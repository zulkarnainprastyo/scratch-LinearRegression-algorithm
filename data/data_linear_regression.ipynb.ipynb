{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1fDQnZLyOro",
        "outputId": "3ef986b2-d7f2-403a-8aad-061c9a8f4055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.22.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!!pip install PyMuPDF\n",
        "!pip install --upgrade PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the PDF content using urllib\n",
        "    pdf_file = urlopen(url).read()\n",
        "\n",
        "    # Create a PDF document object\n",
        "    pdf_document = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "    # Display some metadata information\n",
        "    print(\"PDF Document Metadata:\")\n",
        "    print(f\"Title: {pdf_document.metadata['title']}\")\n",
        "    print(f\"Author: {pdf_document.metadata['author']}\")\n",
        "    print(f\"Subject: {pdf_document.metadata['subject']}\")\n",
        "    print(f\"Creation Date: {pdf_document.metadata['creationDate']}\")\n",
        "    print(f\"Modification Date: {pdf_document.metadata['modDate']}\")\n",
        "\n",
        "except HTTPError as e:\n",
        "    print(f\"HTTP Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGdgfvHCyS9u",
        "outputId": "e53dc2ed-3caa-4bc2-c12e-1fcf37d6569b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: name 'fitz' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "import io\n",
        "from urllib.request import urlopen\n",
        "import PyPDF2\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "# Disable SSL certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# URL of the PDF document\n",
        "url = \"https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf\"\n",
        "\n",
        "try:\n",
        "    # Load the PDF content using urllib\n",
        "    pdf_file = urlopen(url).read()\n",
        "\n",
        "    # Create a PDF reader object\n",
        "    pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))\n",
        "\n",
        "    # Display the number of pages in the PDF\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "except HTTPError as e:\n",
        "    print(f\"Error loading PDF: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zJFDve6yUa0",
        "outputId": "1273536b-bcf9-4c95-ef82-eb3c83dad7dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in the PDF: 406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "# URL of the PDF document\n",
        "url = \"https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf\"\n",
        "\n",
        "try:\n",
        "    # Load the PDF content using urllib\n",
        "    pdf_file = urlopen(url).read()\n",
        "\n",
        "    # Now you can process the PDF content as needed\n",
        "    # For example, you can create a PDF reader object and display the number of pages\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Ir4zNbliyWLi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "!pip install frontend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr0-LsgN0ti6",
        "outputId": "32d8d853-5433-40fb-f3df-9b693d5bc390"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fitz in /usr/local/lib/python3.10/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.8)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (from fitz) (6.0.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (4.0.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.10/dist-packages (from fitz) (1.8.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (1.5.3)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.10/dist-packages (from fitz) (1.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->fitz) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (67.7.2)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.6)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.1)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.0.0)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (7.0.0)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.19.1)\n",
            "Requirement already satisfied: traits!=5.0,<6.4,>=4.6 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (6.3.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.12.2)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: looseversion in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2023.3)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (0.18.3)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.31.0)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.10/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2023.7.22)\n",
            "Requirement already satisfied: frontend in /usr/local/lib/python3.10/dist-packages (0.0.3)\n",
            "Requirement already satisfied: starlette>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.31.0)\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.23.2)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.1.2)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from frontend) (23.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.6)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "import io\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "import fitz  # Import PyMuPDF as fitz\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "# Disable SSL certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Create the 'static' directory if it doesn't exist\n",
        "static_dir = 'static'\n",
        "if not os.path.exists(static_dir):\n",
        "    os.makedirs(static_dir)\n",
        "\n",
        "# URL of the PDF document\n",
        "url = \"https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf\"\n",
        "\n",
        "try:\n",
        "    # Load the PDF content using urllib\n",
        "    pdf_file = urlopen(url).read()\n",
        "\n",
        "    # Create a PDF document object\n",
        "    pdf_document = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "    # Display some metadata information\n",
        "    print(\"PDF Document Metadata:\")\n",
        "    print(f\"Title: {pdf_document.metadata['title']}\")\n",
        "    print(f\"Author: {pdf_document.metadata['author']}\")\n",
        "    print(f\"Subject: {pdf_document.metadata['subject']}\")\n",
        "    print(f\"Creation Date: {pdf_document.metadata['creationDate']}\")\n",
        "    print(f\"Modification Date: {pdf_document.metadata['modDate']}\")\n",
        "\n",
        "except HTTPError as e:\n",
        "    print(f\"Error loading PDF: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE-LFQij0vpH",
        "outputId": "7a92333d-428c-43eb-c1db-3d51e9b5f630"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Document Metadata:\n",
            "Title: \n",
            "Author: \n",
            "Subject: \n",
            "Creation Date: D:20191102163110-04'00'\n",
            "Modification Date: D:20191102163110-04'00'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "import io\n",
        "from urllib.request import urlopen\n",
        "import fitz  # Import PyMuPDF as fitz\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "# Disable SSL certificate verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# URL of the PDF document\n",
        "url = \"https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf\"\n",
        "\n",
        "try:\n",
        "    # Load the PDF content using urllib\n",
        "    pdf_file = urlopen(url).read()\n",
        "\n",
        "    # Create a PDF document object\n",
        "    pdf_document = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "\n",
        "    # Extract text from each page and print it\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document[page_num]\n",
        "        text = page.get_text()\n",
        "        print(f\"Page {page_num + 1}:\\n{text}\\n\")\n",
        "\n",
        "except HTTPError as e:\n",
        "    print(f\"Error loading PDF: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSoEK9eM0ySY",
        "outputId": "f684a8ab-f6a2-4004-eba5-850c2449b1af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming akan dipotong hingga 5000 baris terakhir.\u001b[0m\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "−40\n",
            "−20\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "x\n",
            "y\n",
            "# Plot the data\n",
            "plot(x, y)\n",
            "# Plot the true regression line\n",
            "abline(a = 3, b = -2, col = \"grey\")\n",
            "# Fit by ordinary least squares\n",
            "fit.ols = lm(y ~ x)\n",
            "# Plot that line\n",
            "abline(fit.ols, lty = \"dashed\")\n",
            "FIGURE 21.2: Scatter-plot of n = 150 data points from the above model. (Here X is Gaussian with\n",
            "mean 0 and variance 9.) Grey: True regression line. Dashed: ordinary least squares regression line.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 349:\n",
            "349\n",
            "21.2. HETEROSKEDASTICITY\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "−20\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "x\n",
            "residuals(fit.ols)\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "GG G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "x\n",
            "(residuals(fit.ols))^2\n",
            "par(mfrow = c(1, 2))\n",
            "plot(x, residuals(fit.ols))\n",
            "plot(x, (residuals(fit.ols))^2)\n",
            "par(mfrow = c(1, 1))\n",
            "FIGURE 21.3: Residuals (left) and squared residuals (right) of the ordinary least squares regression\n",
            "as a function of x. Note the much greater range of the residuals at large absolute values of x than\n",
            "towards the center; this changing dispersion is a sign of heteroskedasticity.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 350:\n",
            "21.2. HETEROSKEDASTICITY\n",
            "350\n",
            "# Generate more random samples from the same model and the same x values,\n",
            "# but different y values Inputs: number of samples to generate Presumes: x\n",
            "# exists and is defined outside this function Outputs: errors in linear\n",
            "# regression estimates\n",
            "ols.heterosked.example = function(n) {\n",
            "y = 3 - 2 * x + rnorm(n, 0, sapply(x, function(x) {\n",
            "1 + 0.5 * x^2\n",
            "}))\n",
            "fit.ols = lm(y ~ x)\n",
            "# Return the errors\n",
            "return(fit.ols$coefficients - c(3, -2))\n",
            "}\n",
            "# Calculate average-case errors in linear regression estimates (SD of slope\n",
            "# and intercept) Inputs: number of samples per replication, number of\n",
            "# replications (defaults to 10,000) Calls: ols.heterosked.example Outputs:\n",
            "# standard deviation of intercept and slope\n",
            "ols.heterosked.error.stats = function(n, m = 10000) {\n",
            "ols.errors.raw = t(replicate(m, ols.heterosked.example(n)))\n",
            "# transpose gives us a matrix with named columns\n",
            "intercept.sd = sd(ols.errors.raw[, \"(Intercept)\"])\n",
            "slope.sd = sd(ols.errors.raw[, \"x\"])\n",
            "return(list(intercept.sd = intercept.sd, slope.sd = slope.sd))\n",
            "}\n",
            "FIGURE 21.4: Functions to generate heteroskedastic data and ﬁt OLS regression to it, and to collect\n",
            "error statistics on the results.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 351:\n",
            "351\n",
            "21.2. HETEROSKEDASTICITY\n",
            "0.5 versus 0.25. Since the intercept is ﬁxed by the need to make the regression line go\n",
            "through the center of the data, the real issue here is that our estimate of the slope is\n",
            "much less precise than ordinary least squares makes it out to be. Our estimate is still\n",
            "consistent, but not as good as it was when things were homoskedastic. Can we get\n",
            "back some of that efﬁciency?\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 352:\n",
            "21.2. HETEROSKEDASTICITY\n",
            "352\n",
            "FIGURE 21.5: Statistician (right) consulting the Oracle of Regression (left) about the proper weights\n",
            "to use to overcome heteroskedasticity. (Image from http://en.wikipedia.org/wiki/Image:Pythia1.jpg.)\n",
            "21.2.1\n",
            "Weighted Least Squares as a Solution to Heteroskedasticity\n",
            "Suppose we visit the Oracle of Regression (Figure 21.5), who tells us that the noise\n",
            "has a standard deviation that goes as 1 + x2/2. We can then use this to improve our\n",
            "regression, by solving the weighted least squares problem rather than ordinary least\n",
            "squares (Figure 21.6).\n",
            "The estimated line is now 2.92−2.38x, with reported standard errors of 0.34 and\n",
            "0.19. Does this check out with simulation? (Figure 21.7.)\n",
            "Unsurprisingly, yes. The standard errors from the simulation are 0.34 for the\n",
            "intercept and 0.19 for the slope, so R’s internal calculations are working very well.\n",
            "Why does putting these weights into WLS improve things?\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 353:\n",
            "353\n",
            "21.2. HETEROSKEDASTICITY\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "−40\n",
            "−20\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "x\n",
            "y\n",
            "# Plot the data\n",
            "plot(x, y)\n",
            "# Plot the true regression line\n",
            "abline(a = 3, b = -2, col = \"grey\")\n",
            "# Fit by ordinary least squares\n",
            "fit.ols = lm(y ~ x)\n",
            "# Plot that line\n",
            "abline(fit.ols, lty = \"dashed\")\n",
            "fit.wls = lm(y ~ x, weights = 1/(1 + 0.5 * x^2))\n",
            "abline(fit.wls, lty = \"dotted\")\n",
            "FIGURE 21.6: Figure 21.2, plus the weighted least squares regression line (dotted).\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 354:\n",
            "21.2. HETEROSKEDASTICITY\n",
            "354\n",
            "### As previous two functions, but with weighted regression\n",
            "# Generate random sample from model (with fixed x), fit by weighted least\n",
            "# squares Inputs: number of samples Presumes: x fixed outside function\n",
            "# Outputs: errors in parameter estimates\n",
            "wls.heterosked.example = function(n) {\n",
            "y = 3 - 2 * x + rnorm(n, 0, sapply(x, function(x) {\n",
            "1 + 0.5 * x^2\n",
            "}))\n",
            "fit.wls = lm(y ~ x, weights = 1/(1 + 0.5 * x^2))\n",
            "# Return the errors\n",
            "return(fit.wls$coefficients - c(3, -2))\n",
            "}\n",
            "# Calculate standard errors in parameter estiamtes over many replications\n",
            "# Inputs: number of samples per replication, number of replications\n",
            "# (defaults to 10,000) Calls: wls.heterosked.example Outputs: standard\n",
            "# deviation of estimated intercept and slope\n",
            "wls.heterosked.error.stats = function(n, m = 10000) {\n",
            "wls.errors.raw = t(replicate(m, wls.heterosked.example(n)))\n",
            "# transpose gives us a matrix with named columns\n",
            "intercept.sd = sd(wls.errors.raw[, \"(Intercept)\"])\n",
            "slope.sd = sd(wls.errors.raw[, \"x\"])\n",
            "return(list(intercept.sd = intercept.sd, slope.sd = slope.sd))\n",
            "}\n",
            "FIGURE 21.7: Linear regression of heteroskedastic data, using weighted least-squared regression.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 355:\n",
            "355\n",
            "21.2. HETEROSKEDASTICITY\n",
            "21.2.2\n",
            "Some Explanations for Weighted Least Squares\n",
            "Qualitatively, the reason WLS with inverse variance weights works is the following.\n",
            "OLS cares equally about the error at each data point.1 Weighted least squares, natu-\n",
            "rally enough, tries harder to match observations where the weights are big, and less\n",
            "hard to match them where the weights are small. But each yi contains not only the\n",
            "true regression function m(xi) but also some noise εi. The noise terms have large\n",
            "magnitudes where the variance is large. So we should want to have small weights\n",
            "where the noise variance is large, because there the data tends to be far from the true\n",
            "regression. Conversely, we should put big weights where the noise variance is small,\n",
            "and the data points are close to the true regression.\n",
            "The qualitative reasoning in the last paragraph doesn’t explain why the weights\n",
            "should be inversely proportional to the variances, wi ∝ 1/σ2\n",
            "i — why not wi ∝ 1/σi,\n",
            "for instance? Look at the equation for the WLS estimates again:\n",
            "�\n",
            "βW LS = (xT wx)−1xT wy\n",
            "(21.7)\n",
            "Imagine holding x constant, but repeating the experiment multiple times, so that we\n",
            "get noisy values of y. In each experiment, Yi = xi·β + εi, where �[εi|x] = 0 and\n",
            "Var[εi|x] = σ2\n",
            "i . So\n",
            "�\n",
            "βW LS\n",
            "=\n",
            "(xT wx)−1xT wxβ + (xT wx)−1xT wε\n",
            "(21.8)\n",
            "=\n",
            "β + (xT wx)−1xT wε\n",
            "(21.9)\n",
            "Since �[ε|x] = 0, the WLS estimator is unbiased:\n",
            "�\n",
            "� �\n",
            "βW LS|x\n",
            "�\n",
            "= β\n",
            "(21.10)\n",
            "In fact, for the j th coefﬁcient,\n",
            "�\n",
            "βj\n",
            "=\n",
            "βj + [(xT wx)−1xT wε]j\n",
            "(21.11)\n",
            "=\n",
            "βj +\n",
            "n\n",
            "�\n",
            "i=1\n",
            "kj i(w)εi\n",
            "(21.12)\n",
            "where in the last line I have bundled up (xT wx)−1xT w as a matrix k(w), with the\n",
            "argument to remind us that it depends on the weights. Since the WLS estimate is\n",
            "unbiased, it’s natural to want it to also have a small variance, and\n",
            "Var\n",
            "� �\n",
            "βj\n",
            "�\n",
            "=\n",
            "n\n",
            "�\n",
            "i=1\n",
            "kj i(w)σ2\n",
            "i\n",
            "(21.13)\n",
            "It can be shown — the result is called the generalized Gauss-Markov theorem — that\n",
            "picking weights to minimize the variance in the WLS estimate has the unique solution\n",
            "1Less anthropomorphically, the objective function in Eq. 21.1 has the same derivative with respect to\n",
            "the squared error at each point, ∂ MSE\n",
            "∂ e2\n",
            "i\n",
            "= 1\n",
            "n .\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 356:\n",
            "21.3. THE GAUSS-MARKOV THEOREM\n",
            "356\n",
            "wi = 1/σ2\n",
            "i . It does not require us to assume the noise is Gaussian, but the proof does\n",
            "need a few tricks (see §21.3).\n",
            "A less general but easier-to-grasp result comes from adding the assumption that\n",
            "the noise around the regression line is Gaussian — that\n",
            "Y = β0 + β1X1 + ... + βpXp + ε, ε ∼ � (0,σ2\n",
            "x)\n",
            "(21.14)\n",
            "The log-likelihood is then (Exercise 2)\n",
            "− n\n",
            "2 ln2π − 1\n",
            "2\n",
            "n\n",
            "�\n",
            "i=1\n",
            "logσ2\n",
            "i − 1\n",
            "2\n",
            "n\n",
            "�\n",
            "i=1\n",
            "(yi − xi·b)2\n",
            "σ2\n",
            "i\n",
            "(21.15)\n",
            "If we maximize this with respect to β, everything except the ﬁnal sum is irrelevant,\n",
            "and so we minimize\n",
            "n\n",
            "�\n",
            "i=1\n",
            "(yi − xi·b)2\n",
            "σ2\n",
            "i\n",
            "(21.16)\n",
            "which is just weighted least squares with wi = 1/σ2\n",
            "i . So, if the probabilistic assump-\n",
            "tion holds, WLS is the efﬁcient maximum likelihood estimator.\n",
            "21.3\n",
            "The Gauss-Markov Theorem\n",
            "We’ve seen that when we do weighted least squares, our estimates of β are linear in\n",
            "Y, and unbiased (Eq. 21.10):\n",
            "�\n",
            "βW LS\n",
            "=\n",
            "(xT wx)−1xT wy\n",
            "(21.17)\n",
            "�\n",
            "�\n",
            "�\n",
            "βW LS\n",
            "�\n",
            "=\n",
            "β\n",
            "(21.18)\n",
            "What we’d like to show is that using the weights wi = 1/σ2\n",
            "i is somehow optimal.\n",
            "Like any optimality result, it is crucial to lay out carefully the range of possible alter-\n",
            "natives, and the criterion by which those alternatives will be compared. The classical\n",
            "optimality result for estimating linear models is the Gauss-Markov theorem, which\n",
            "takes the range of possibilities to be linear, unbiased estimators of β, and the criterion\n",
            "to be variance of the estimator. I will return to both these choices at the end of this\n",
            "section.\n",
            "Any linear estimator, say �\n",
            "β, could be written as\n",
            "�\n",
            "β = qy\n",
            "where q would be a (p + 1) × n matrix, in general a function of x, weights, the phase\n",
            "of the moon, etc. (For OLS, q = (xT x)−1xT .) For �\n",
            "β to be an unbiased estimator, we\n",
            "must have\n",
            "�[qY|x] = qxβ = β\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 357:\n",
            "357\n",
            "21.3. THE GAUSS-MARKOV THEOREM\n",
            "Since this must hold for all β and all x, we have to have qx = I.2 (Sanity check: this\n",
            "works for OLS.) The variance is then\n",
            "Var[qY|x] = qVar[ε|x]qT = qΣq\n",
            "(21.19)\n",
            "where I abbreviate the mouthful Var[ε|x] by Σ. We could then try to differentiate\n",
            "this with respect to q, set the derivative to zero, and solve, but this gets rather messy,\n",
            "since in addition to the complications of matrix calculus, we’d need to enforce the\n",
            "unbiasedness constraint qx = I somehow.\n",
            "Instead of the direct approach, we’ll use a classic piece of trickery. Set\n",
            "k ≡ (xT Σ−1x)−1xT Σ−1\n",
            "which is the estimating matrix for weighted least squares. Now, whatever q might be,\n",
            "we can always write\n",
            "q = k + r\n",
            "(21.20)\n",
            "for some matrix r. The unbiasedness constraint on q translates into\n",
            "rx = 0\n",
            "because kx = I. Now we substitute Eq. 21.20 into Eq. 21.19:\n",
            "Var\n",
            "� �\n",
            "β\n",
            "�\n",
            "=\n",
            "(k + r)Σ(k + r)T\n",
            "(21.21)\n",
            "=\n",
            "(k + r)Σ−1(k + r)T\n",
            "(21.22)\n",
            "=\n",
            "kΣkT + rΣkT + kΣrT + rΣrT\n",
            "(21.23)\n",
            "=\n",
            "(xT Σ−1x)−1xT Σ−1ΣΣ−1x(xT Σ−1x)−1\n",
            "(21.24)\n",
            "+rΣΣ−1x(xT Σ−1x)−1\n",
            "+(xT Σ−1x)−1xT Σ−1ΣrT\n",
            "+rΣrT\n",
            "=\n",
            "(xT Σ−1x)−1xT Σ−1x(xT Σ−1x)−1\n",
            "(21.25)\n",
            "+rx(xT Σ−1x)−1 + (xT Σ−1x)−1xT rT\n",
            "+rΣrT\n",
            "=\n",
            "(xT Σ−1x)−1 + rΣrT\n",
            "(21.26)\n",
            "where the last step uses the fact that rx = 0 (and so xT rT = 0T ).\n",
            "Since Σ is a covariance matrix, it’s positive deﬁnite, meaning that aΣaT ≥ 0 for\n",
            "any vector a. This applies in particular to the vector ri·, i.e., the ith row of r. But\n",
            "Var\n",
            "� ˜βi\n",
            "�\n",
            "= (xT Σ−1x)−1\n",
            "ii + ri·w0\n",
            "−1rT\n",
            "i·\n",
            "which must therefore be strictly larger than (xT Σ−1x)−1\n",
            "ii , the variance we’d get from\n",
            "using weighted least squares.\n",
            "2This doesn’t mean that q = x−1; x doesn’t have an inverse!\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 358:\n",
            "21.4. FINDING THE VARIANCE AND WEIGHTS\n",
            "358\n",
            "FIGURE 21.8: The Oracle may be out (left), or too creepy to go visit (right). What then? (Left,\n",
            "the sacred oak of the Oracle of Dodona, copyright 2006 by Flickr user “essayen”, http://flickr.\n",
            "com/photos/essayen/245236125/; right, the entrace to the cave of the Sibyl of Cumæ, copyright\n",
            "2005 by Flickr user “pverdicchio”, http://flickr.com/photos/occhio/17923096/. Both used\n",
            "under Creative Commons license.)\n",
            "We conclude that WLS, with the weight matrix w equal to the inverse variance\n",
            "matrix Σ−1, the least variance among all possible linear, unbiased estimators of the\n",
            "regression coefﬁcients.\n",
            "Notes:\n",
            "1. If all the noise variances are equal, then we’ve proved the optimality of OLS.\n",
            "2. The theorem doesn’t rule out linear, biased estimators with smaller variance. As\n",
            "an example, albeit a trivial one, 0y is linear and has variance 0, but is (generally)\n",
            "very biased.\n",
            "3. The theorem also doesn’t rule out non-linear unbiased estimators of smaller\n",
            "variance. Or indeed non-linear biased estimators of even smaller variance.\n",
            "4. The proof actually doesn’t require the variance matrix to be diagonal.\n",
            "21.4\n",
            "Finding the Variance and Weights\n",
            "All of this was possible because the Oracle told us what the variance function was.\n",
            "What do we do when the Oracle is not available (Figure 21.8)?\n",
            "Sometimes we can work things out for ourselves, without needing an oracle.\n",
            "• We know, empirically, the precision of our measurement of the response vari-\n",
            "able — we know how precise our instruments are, or the response is really an\n",
            "average of several measurements so we can use their standard deviations, etc.\n",
            "• We know how the noise in the response must depend on the input variables.\n",
            "For example, when taking polls or surveys, the variance of the proportions we\n",
            "ﬁnd should be inversely proportional to the sample size. So we can make the\n",
            "weights proportional to the sample size.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 359:\n",
            "359\n",
            "21.4. FINDING THE VARIANCE AND WEIGHTS\n",
            "Both of these outs rely on kinds of background knowledge which are easier to\n",
            "get in the natural or even the social sciences than in many industrial applications.\n",
            "However, there are approaches for other situations which try to use the observed\n",
            "residuals to get estimates of the heteroskedasticity; this is the topic of the next section.\n",
            "21.4.1\n",
            "Variance Based on Probability Considerations\n",
            "There are a number of situations where we can reasonably base judgments of variance,\n",
            "or measurement variance, on elementary probability.\n",
            "Multiple measurements\n",
            "The easiest case is when our measurements of the response\n",
            "are actually averages over individual measurements, each with some variance σ2. If\n",
            "some Yi are based on averaging more individual measurements than others, there will\n",
            "be heteroskedasticity. The variance of the average of ni uncorrelated measurements\n",
            "will be σ2/ni, so in this situation we could take wi ∝ ni.\n",
            "Binomial counts\n",
            "Suppose our response variable is a count, derived from a binomial\n",
            "distribution, i.e., Yi ∼ Binom(ni, pi). We would usually model pi as a function of\n",
            "the predictor variables — at this level of statistical knowledge, a linear function. This\n",
            "would imply that Yi had expectation ni pi, and variance ni pi(1 − pi). We would\n",
            "be well-advised to use this formula for the variance, rather than pretending that all\n",
            "observations had equal variance.\n",
            "Proportions based on binomials\n",
            "If our response variable is a proportion based on\n",
            "a binomial, we’d see an expectation value of pi and a variance of pi(1−pi)\n",
            "ni\n",
            ". Again, this\n",
            "is not equal across different values of ni, or for that matter different values of pi.\n",
            "Poisson counts\n",
            "Binomial counts have a hard upper limit, ni; if the upper limit is\n",
            "immense or even (theoretically) inﬁnite, we may be better off using a Poisson distribu-\n",
            "tion. In such situations, the mean of the Poisson λi will be a (possibly-linear) function\n",
            "of the predictors, and the variance will also be equal to λi.\n",
            "Other counts\n",
            "The binomial and Poisson distributions rest on independence across\n",
            "“trials” (whatever those might be). There are a range of discrete probability models\n",
            "which allow for correlation across trials (leadings to more or less variance). These\n",
            "may, in particular situations, be more appropriate.\n",
            "21.4.1.1\n",
            "Example: The Economic Mobility Data\n",
            "The data set on economic mobility we’ve used in a number of assignments and exam-\n",
            "ples actually contains a bunch of other variables in addition to the covariates we’ve\n",
            "looked at (short commuting times and latitude and longitude). While reserving the\n",
            "full data set for later use, let’s look one of the additional covariates, namely popula-\n",
            "tion.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 360:\n",
            "21.4. FINDING THE VARIANCE AND WEIGHTS\n",
            "360\n",
            "To see why this might be relevant, recall that our response variable is the fraction\n",
            "of children who, in each community, were born into the lowest 20% of the income dis-\n",
            "tribution during 1980–1982 and nonetheless make it into the top 20% by age 30; we’re\n",
            "looking at a proportion. Different communities will have had different numbers of\n",
            "children born in the relevant period, generally proportional to their total population.\n",
            "Treating the observed fraction for New York City as being just as far from its expected\n",
            "rate of mobility as that for Pifﬂeburg, WI is asking for trouble.\n",
            "Once we have population, there is a very notable pattern: the most extreme levels\n",
            "of mobility are all for very small communities (Figure 21.9).\n",
            "While we do not know the exact number of children for each community, it is\n",
            "not unreasonable to take that as proportional to the total population. The binomial\n",
            "standard error in the observed fraction will therefore be ∝\n",
            "�\n",
            "pi(1−pi)\n",
            "ni\n",
            ".\n",
            "mobility$MobSE <- with(mobility, sqrt(Mobility * (1 - Mobility)/Population))\n",
            "Let us now plot the rate of economic mobility against the fraction of workers\n",
            "with short commutes, and decorate it with error bars reﬂecting these standard errors\n",
            "(Figure 21.10).\n",
            "Now, there are reasons why this is not necessarily the last word on using weighted\n",
            "least squares here. One is that if we actually believed our model, we should be using\n",
            "the predicted mobility as the pi in\n",
            "�\n",
            "pi(1−pi)\n",
            "ni\n",
            ", rather than the observed mobility. An-\n",
            "other is that the binomial model assumes independence across “trials” (here, children).\n",
            "But, by deﬁnition, at most, and at least, 20% of the population ends up in the top 20%\n",
            "of the income distribution3. It’s fairly clear, however, that simply ignoring differences\n",
            "in the sizes of communities is unwise.\n",
            "3Cf. Gore Vidal: “It is not enough to succeed; others must also fail.”\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 361:\n",
            "361\n",
            "21.4. FINDING THE VARIANCE AND WEIGHTS\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "5e+03\n",
            "5e+04\n",
            "5e+05\n",
            "5e+06\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "Population\n",
            "Mobility\n",
            "mobility <- read.csv(\"http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/24--25/mobility2.csv\")\n",
            "plot(Mobility ~ Population, data = mobility, log = \"x\", ylim = c(0, 0.5))\n",
            "FIGURE 21.9: Rate of economic mobility plotted against population, with a logarithmic scale on\n",
            "the latter, horizontal axis. Notice decreasing spread at larger population.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 362:\n",
            "21.4. FINDING THE VARIANCE AND WEIGHTS\n",
            "362\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "0.8\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "Fraction of workers with short commutes\n",
            "Rate of economic mobility\n",
            "plot(Mobility ~ Commute, data = mobility, xlab = \"Fraction of workers with short commutes\",\n",
            "ylab = \"Rate of economic mobility\", pch = 19, cex = 0.2)\n",
            "with(mobility, segments(x0 = Commute, y0 = Mobility + 2 * MobSE, x1 = Commute,\n",
            "y1 = Mobility - 2 * MobSE, col = \"blue\"))\n",
            "mob.lm <- lm(Mobility ~ Commute, data = mobility)\n",
            "mob.wlm <- lm(Mobility ~ Commute, data = mobility, weight = 1/MobSE^2)\n",
            "abline(mob.lm)\n",
            "abline(mob.wlm, col = \"blue\")\n",
            "FIGURE 21.10: Mobility versus the fraction of workers with short commute, with ±2 standard\n",
            "deviation error bars (vertical blue bars), and the OLS linear ﬁt (black line) and weighted least squares\n",
            "(blue line). Note that the error bars for some larger communities are smaller than the diameter of\n",
            "the dots.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 363:\n",
            "363\n",
            "21.5. CONDITIONAL VARIANCE FUNCTION ESTIMATION\n",
            "21.5\n",
            "Conditional Variance Function Estimation\n",
            "Remember that there are two equivalent ways of deﬁning the variance:\n",
            "Var[X] = �\n",
            "�\n",
            "X 2�\n",
            "− (�[X])2 = �\n",
            "�\n",
            "(X − �[X])2�\n",
            "(21.27)\n",
            "The latter is more useful for us when it comes to estimating variance functions. We\n",
            "have already ﬁgured out how to estimate means — that’s what all this previous work\n",
            "on smoothing and regression is for — and the deviation of a random variable from its\n",
            "mean shows up as a residual.\n",
            "There are two generic ways to estimate conditional variances, which differ slightly\n",
            "in how they use non-parametric smoothing. We can call these the squared residuals\n",
            "method and the log squared residuals method. Here is how the ﬁrst one goes.\n",
            "1. Estimate m(x) with your favorite regression method, getting ˆm(x).\n",
            "2. Construct the squared residuals, ui = (yi − ˆm(xi))2.\n",
            "3. Use your favorite non-parametric method to estimate the conditional mean of\n",
            "the ui, call it �q(x).\n",
            "4. Predict the variance using �σ2\n",
            "x = �q(x).\n",
            "The log-squared residuals method goes very similarly.4\n",
            "1. Estimate m(x) with your favorite regression method, getting ˆm(x).\n",
            "2. Construct the log squared residuals, zi = log(yi − ˆm(xi))2.\n",
            "3. Use your favorite non-parametric method to estimate the conditional mean of\n",
            "the zi, call it ˆs(x).\n",
            "4. Predict the variance using �σ2\n",
            "x = exp�s(x).\n",
            "The quantity yi − ˆm(xi) is the ith residual. If �\n",
            "m ≈ m, then the residuals should\n",
            "have mean zero. Consequently the variance of the residuals (which is what we want)\n",
            "should equal the expected squared residual. So squaring the residuals makes sense, and\n",
            "the ﬁrst method just smoothes these values to get at their expectations.\n",
            "What about the second method — why the log? Basically, this is a convenience\n",
            "— squares are necessarily non-negative numbers, but lots of regression methods don’t\n",
            "easily include constraints like that, and we really don’t want to predict negative vari-\n",
            "ances.5 Taking the log gives us an unbounded range for the regression.\n",
            "Strictly speaking, we don’t need to use non-parametric smoothing for either method.\n",
            "If we had a parametric model for σ2\n",
            "x, we could just ﬁt the parametric model to the\n",
            "squared residuals (or their logs). But even if you think you know what the variance\n",
            "function should look like it, why not check it?\n",
            "4I learned it from Wasserman (2006, pp. 87–88).\n",
            "5Occasionally people do things like claiming that gene differences explains more than 100% of the\n",
            "variance in some psychological trait, and so environment and up-bringing contribute negative variance.\n",
            "Some of them — like Alford et al. (2005) — even say this with a straight face.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 364:\n",
            "21.5. CONDITIONAL VARIANCE FUNCTION ESTIMATION\n",
            "364\n",
            "We came to estimating the variance function because of wanting to do weighted\n",
            "least squares, but these methods can be used more generally. It’s often important to\n",
            "understand variance in its own right, and this is a general method for estimating it.\n",
            "Our estimate of the variance function depends on ﬁrst having a good estimate of the\n",
            "regression function\n",
            "21.5.1\n",
            "Iterative Reﬁnement of Mean and Variance: An Example\n",
            "The estimate �σ2\n",
            "x depends on the initial estimate of the regression function ˆm(x). But,\n",
            "as we saw when we looked at weighted least squares, taking heteroskedasticity into\n",
            "account can change our estimates of the regression function. This suggests an itera-\n",
            "tive approach, where we alternate between estimating the regression function and the\n",
            "variance function, using each to improve the other. That is, we take either method\n",
            "above, and then, once we have estimated the variance function �σ2\n",
            "x, we re-estimate ˆm\n",
            "using weighted least squares, with weights inversely proportional to our estimated\n",
            "variance. Since this will generally change our estimated regression, it will change the\n",
            "residuals as well. Once the residuals have changed, we should re-estimate the variance\n",
            "function. We keep going around this cycle until the change in the regression function\n",
            "becomes so small that we don’t care about further modiﬁcations. It’s hard to give a\n",
            "strict guarantee, but usually this sort of iterative improvement will converge.\n",
            "Let’s apply this idea to our example. Figure 21.3b already plotted the residuals\n",
            "from OLS. Figure 21.11 shows those squared residuals again, along with the true vari-\n",
            "ance function and the estimated variance function.\n",
            "The OLS estimate of the regression line is not especially good ( �\n",
            "β0 = 2.59 versus\n",
            "β0 = 3, �\n",
            "β1 = −3.18 versus β1 = −2), so the residuals are systematically off, but it’s\n",
            "clear from the ﬁgure that spline smoothing of the squared residuals is picking up on the\n",
            "heteroskedasticity, and getting a pretty reasonable picture of the variance function.\n",
            "Now we use the estimated variance function to re-estimate the regression line,\n",
            "with weighted least squares.\n",
            "fit.wls1 <- lm(y ~ x, weights = 1/exp(var1$y))\n",
            "coefficients(fit.wls1)\n",
            "## (Intercept)\n",
            "x\n",
            "##\n",
            "2.211184\n",
            "-2.978369\n",
            "var2 <- smooth.spline(x = x, y = log(residuals(fit.wls1)^2), cv = TRUE)\n",
            "The slope has changed substantially, and in the right direction (Figure 21.12a). The\n",
            "residuals have also changed (Figure 21.12b), and the new variance function is closer to\n",
            "the truth than the old one.\n",
            "Since we have a new variance function, we can re-weight the data points and re-\n",
            "estimate the regression:\n",
            "fit.wls2 <- lm(y ~ x, weights = 1/exp(var2$y))\n",
            "coefficients(fit.wls2)\n",
            "## (Intercept)\n",
            "x\n",
            "##\n",
            "2.243152\n",
            "-3.041736\n",
            "var3 <- smooth.spline(x = x, y = log(residuals(fit.wls2)^2), cv = TRUE)\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 365:\n",
            "365\n",
            "21.5. CONDITIONAL VARIANCE FUNCTION ESTIMATION\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "x\n",
            "squared residuals\n",
            "plot(x, residuals(fit.ols)^2, ylab = \"squared residuals\")\n",
            "curve((1 + x^2/2)^2, col = \"grey\", add = TRUE)\n",
            "var1 <- smooth.spline(x = x, y = log(residuals(fit.ols)^2), cv = TRUE)\n",
            "grid.x <- seq(from = min(x), to = max(x), length.out = 300)\n",
            "lines(grid.x, exp(predict(var1, x = grid.x)$y))\n",
            "FIGURE 21.11: Points: actual squared residuals from the OLS line. Grey curve: true variance\n",
            "function, σ2\n",
            "x = (1 + x2/2)2. Black curve: spline smoothing of the squared residuals.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 366:\n",
            "21.5. CONDITIONAL VARIANCE FUNCTION ESTIMATION\n",
            "366\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "−40\n",
            "−20\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "x\n",
            "y\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "GG G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "GG\n",
            "G\n",
            "GG\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "G\n",
            "−5\n",
            "0\n",
            "5\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "x\n",
            "squared residuals\n",
            "fit.wls1 <- lm(y ~ x, weights = 1/exp(var1$y))\n",
            "par(mfrow = c(1, 2))\n",
            "plot(x, y)\n",
            "abline(a = 3, b = -2, col = \"grey\")\n",
            "abline(fit.ols, lty = \"dashed\")\n",
            "abline(fit.wls1, lty = \"dotted\")\n",
            "plot(x, (residuals(fit.ols))^2, ylab = \"squared residuals\")\n",
            "points(x, residuals(fit.wls1)^2, pch = 15)\n",
            "lines(grid.x, exp(predict(var1, x = grid.x)$y))\n",
            "var2 <- smooth.spline(x = x, y = log(residuals(fit.wls1)^2), cv = TRUE)\n",
            "curve((1 + x^2/2)^2, col = \"grey\", add = TRUE)\n",
            "lines(grid.x, exp(predict(var2, x = grid.x)$y), lty = \"dotted\")\n",
            "par(mfrow = c(1, 1))\n",
            "FIGURE 21.12: Left: As in Figure 21.2, but with the addition of the weighted least squares regression\n",
            "line (dotted), using the estimated variance from Figure 21.11 for weights. Right: As in Figure 21.11,\n",
            "but with the addition of the residuals from the WLS regression (black squares), and the new estimated\n",
            "variance function (dotted curve).\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 367:\n",
            "367\n",
            "21.5. CONDITIONAL VARIANCE FUNCTION ESTIMATION\n",
            "Since we know that the true coefﬁcients are 3 and −2, we know that this is moving\n",
            "in the right direction. If I hadn’t told you what they were, you could still observe that\n",
            "the difference in coefﬁcients between fit.wls1 and fit.wls2 is smaller than that\n",
            "between fit.ols and fit.wls1, which is a sign that this is converging.\n",
            "I will spare you the plot of the new regression and of the new residuals. When we\n",
            "update a few more times:\n",
            "fit.wls3 <- lm(y ~ x, weights = 1/exp(var3$y))\n",
            "coefficients(fit.wls3)\n",
            "## (Intercept)\n",
            "x\n",
            "##\n",
            "2.239878\n",
            "-3.024477\n",
            "var4 <- smooth.spline(x = x, y = log(residuals(fit.wls3)^2), cv = TRUE)\n",
            "fit.wls4 <- lm(y ~ x, weights = 1/exp(var4$y))\n",
            "coefficients(fit.wls4)\n",
            "## (Intercept)\n",
            "x\n",
            "##\n",
            "2.240906\n",
            "-3.027914\n",
            "By now, the coefﬁcients of the regression are changing relatively little, and we\n",
            "only have 150 data points, so the imprecision from a limited sample surely swamps\n",
            "the changes we’re making, and we might as well stop.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 368:\n",
            "21.6. CORRELATED NOISE AND GENERALIZED LEAST SQUARES\n",
            "368\n",
            "Manually going back and forth between estimating the regression function and\n",
            "estimating the variance function is tedious. We could automate it with a function,\n",
            "which would look something like this:\n",
            "iterative.wls <- function(x, y, tol = 0.01, max.iter = 100) {\n",
            "iteration <- 1\n",
            "old.coefs <- NA\n",
            "regression <- lm(y ~ x)\n",
            "coefs <- coefficients(regression)\n",
            "while (is.na(old.coefs) || ((max(abs(coefs - old.coefs)) > tol) && (iteration <\n",
            "max.iter))) {\n",
            "variance <- smooth.spline(x = x, y = log(residuals(regression)^2), cv = TRUE)\n",
            "old.coefs <- coefs\n",
            "iteration <- iteration + 1\n",
            "regression <- lm(y ~ x, weights = 1/exp(variance$y))\n",
            "coefs <- coefficients(regression)\n",
            "}\n",
            "return(list(regression = regression, variance = variance, iterations = iteration))\n",
            "}\n",
            "This starts by doing an unweighted linear regression, and then alternates between\n",
            "WLS for the getting the regression and spline smoothing for getting the variance.\n",
            "It stops when no parameter of the regression changes by more than tol, or when\n",
            "it’s gone around the cycle max.iter times.6 This code is a bit too inﬂexible to be\n",
            "really “industrial strength” (what if we wanted to use a data frame, or a more complex\n",
            "regression formula?), but shows the core idea.\n",
            "21.6\n",
            "Correlated Noise and Generalized Least Squares\n",
            "Sometimes, we might believe the right model is (in matrix form)\n",
            "Y\n",
            "=\n",
            "Xβ + ε\n",
            "(21.28)\n",
            "�[ε|X]\n",
            "=\n",
            "0\n",
            "(21.29)\n",
            "Var[ε|X]\n",
            "=\n",
            "Σ\n",
            "(21.30)\n",
            "where the matrix Σ is not diagonal. The off-diagonal entries represent covariance in\n",
            "the noise terms, Cov\n",
            "�\n",
            "εi,εj\n",
            "�\n",
            "= Σi j. In fact, we should think this is the right model\n",
            "more often than the “usual” linear regression model, which is the special case where\n",
            "Σ = σ2I. There is, after all, no reason in general considerations of probability theory\n",
            "or mathematical modeling to expect that ﬂuctuations around a linear model will be\n",
            "uncorrelated. How might we nonetheless estimate β?\n",
            "One approach is to try to make the noise disappear, by transforming the variables.\n",
            "Suppose we know Σ. (We’ll come back to where such knowledge might come from\n",
            "6The condition in the while loop is a bit complicated, to ensure that the loop is executed at least once.\n",
            "Some languages have an until control structure which would simplify this.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 369:\n",
            "369\n",
            "21.6. CORRELATED NOISE AND GENERALIZED LEAST SQUARES\n",
            "later.) Because Σ is a variance matrix, we know it is square, symmetric, and positive-\n",
            "deﬁnite. This is enough to guarantee7 that there is another square matrix, say s, where\n",
            "ssT = Σ, as it were s =\n",
            "�\n",
            "Σ. I bring this fact up because we can use this to make the\n",
            "correlations in the noise go away.\n",
            "Go back to our model equation, and multiply everything from the left by s−1.\n",
            "s−1Y = s−1Xβ + s−1ε\n",
            "This looks like a linear regression of s−1Y on s−1X, with the same coefﬁcients β as\n",
            "our original regression. However, we have improved the properties of the noise. The\n",
            "noise is still zero in expectation,\n",
            "�\n",
            "�\n",
            "s−1ε|X\n",
            "�\n",
            "= s−10 = 0\n",
            "but the covariance has gone away, and all the noise terms have equal variance:\n",
            "Var\n",
            "�\n",
            "s−1ε|x\n",
            "�\n",
            "=\n",
            "s−1Var[ε|x]s−T\n",
            "(21.31)\n",
            "=\n",
            "s−1Σs−T\n",
            "(21.32)\n",
            "=\n",
            "s−1ssT s−T\n",
            "(21.33)\n",
            "=\n",
            "I\n",
            "(21.34)\n",
            "(This multiplication by s−1 is the equivalent, for random vectors, of dividing a random\n",
            "variable by its standard deviation, to get something with variance 1.)\n",
            "To sum up, if we know Σ, we can estimate β by doing an ordinary least squares\n",
            "regression of s−1Y on s−1X. The estimate is\n",
            "�\n",
            "β\n",
            "=\n",
            "((s−1x)T s−1x)−1(s−1x)T s−1y\n",
            "(21.35)\n",
            "=\n",
            "(xT s−T s−1x)−1xT s−T s−1y\n",
            "(21.36)\n",
            "=\n",
            "(xT Σ−1x)−1xT Σ−1y\n",
            "(21.37)\n",
            "This looks just like our weighted least squares estimate, only with Σ−1 in place of\n",
            "w.\n",
            "21.6.1\n",
            "Generalized Least Squares\n",
            "This resemblance is no mere coincidence. We can write the WLS problem as that of\n",
            "minimizing (y−xβ)T w(y−xβ), for a diagonal matrix w. Suppose we try instead to\n",
            "minimize\n",
            "(y − xβ)T w(y − xβ)\n",
            "for a non-diagonal, but still symmetric and positive-deﬁnite, matrix w. This is called a\n",
            "generalized least squares (GLS) problem. Every single step we went through before\n",
            "is still valid, because none of it rested on w being diagonal, so\n",
            "�\n",
            "βGLS = (xT wx)−1xT wy\n",
            "(21.38)\n",
            "7Here’s one way to do it: invoke the “spectral” or “eigendecomposition” theorem, to write Σ = vλvT ,\n",
            "where v is the matrix whose columns are the eigenvectors of Σ, and λ is the diagonal matrix of the eigen-\n",
            "values of Σ. Then if we set s = v\n",
            "�\n",
            "λ, we’d have Σ = ssT , as desired.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 370:\n",
            "21.7. WLS AND GLS VS. SPECIFICATION ERRORS\n",
            "370\n",
            "What we have just seen is that if we set w = Σ−1, we also get this solution when\n",
            "we transform the variables so as to de-correlate the noise, and then do ordinary least\n",
            "squares. This should at least make it plausible that this is a good way to estimate β in\n",
            "the face of correlated noise.\n",
            "To go beyond plausibility, refer back to §21.3. At no point in our reasoning did\n",
            "we actually rely on Var[ε|x] being diagonal. It follows that if we set w = Var[ε|x]−1,\n",
            "we get the linear, unbiased estimator of minimum variance. If we believe that the\n",
            "noise is Gaussian, then this is also the maximum likelihood estimator.\n",
            "21.6.2\n",
            "Where Do the Covariances Come From?\n",
            "The soundest way to estimate a covariance would be to repeat the experiment many\n",
            "times, under identical conditions. This corresponds to using repeated measurements\n",
            "to estimate variances. It’s simple, it works when we can do it, and there is accordingly\n",
            "little to say about it. Except: there are few situations where we can do it.\n",
            "When we wanted to estimate the variance function, we could take all the squared\n",
            "residuals for values of xi around a given x and use that as an estimate of σ2(x). This\n",
            "option is not available to us when we are looking at covariances.\n",
            "If our measurements are spread out over time or space, it’s natural to suppose that\n",
            "there is more covariance between nearby observations than between remote ones. A\n",
            "stronger but more delicate assumption is that of stationarity, that the covariance be-\n",
            "tween an observation taken at time 0 and time h is the same as the covariance between\n",
            "time t and time t + h, whatever t might be. (And similarly for spatial stationarity.)\n",
            "Call the covariance in at this lag or separation γ(h). We can estimate it by taking pairs\n",
            "of observations where the separation is approximately h, and averaging the products\n",
            "of their residuals.\n",
            "It is common (though perhaps not wise) to make even stronger assumptions, such\n",
            "as that the covariance decays exponentially with distance, γ(h) = γ(0)ρh or γ(h) =\n",
            "γ(0)e−h/τ. When we can believe such assumptions, they let us estimate the parameters\n",
            "of the covariance function using the sample covariances across all lags. The estimated\n",
            "covariance function, using all of that data, is much more stable than having many\n",
            "separate sample covariances, one for each lag. Even if the assumptions are, strictly,\n",
            "false, the stability that comes from forcing all the covariances to follow a common\n",
            "model can be desirable, on bias-variance grounds.\n",
            "21.7\n",
            "WLS and GLS vs. Speciﬁcation Errors\n",
            "When you ﬁnd that your residuals from an initial model have non-constant variance\n",
            "or are correlated with each other, there are (at least) two possible explanations. One is\n",
            "that the ﬂuctuations around the regression line really are heteroskedastic and/or cor-\n",
            "related. In that case, you should try to model that variance and those correlations, and\n",
            "use WLS or GLS. The other explanation is that something is wrong with your model.\n",
            "If there’s an important predictor variable which is just missing from your model, for\n",
            "example, then its contribution to the response will be part of your residuals. If that\n",
            "omitted variable is larger in some parts of the data than in others, or if the omitted\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 371:\n",
            "371\n",
            "21.8. EXERCISES\n",
            "variable has correlations, then that will make your residuals change in magnitude and\n",
            "be correlated. More subtly, having the wrong functional form for a variable you do\n",
            "include can produce those effects as well.\n",
            "21.8\n",
            "Exercises\n",
            "1. Imagine we are trying to estimate the mean value of Y from a large population.\n",
            "We observe n members of the population, with individual i being included in\n",
            "our sample with a probability proportional to πi. Show that the sample mean\n",
            "n−1 �n\n",
            "i=1 yi is not a consistent estimator of �[Y ] unless all the πi are equal.\n",
            "Show that\n",
            "��n\n",
            "i=1 yi/πi\n",
            "�\n",
            "/�n\n",
            "i′=1 1/πi′ is a consistent estimator of �[Y ].\n",
            "2. Show that the model of Eq. 21.14 has the log-likelihood given by Eq. 21.15\n",
            "3. Do the calculus to verify Eq. 21.6.\n",
            "4. Is wi = 1 a necessary as well as a sufﬁcient condition for Eq. 21.3 and Eq. 21.1\n",
            "to have the same minimum?\n",
            "5. §21.2.2 showed that WLS gives better parameter estimates than OLS when there\n",
            "is heteroskedasticity, and we know and use the variance. Modify the code for\n",
            "to see which one has better generalization error.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 372:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Chapter 22\n",
            "Variable Selection\n",
            "22.1\n",
            "What Variable Selection Is\n",
            "“Variable selection” means selecting which variables to include in our model (rather\n",
            "than some sort of selection which is itself variable). As such, it is a special case of\n",
            "model selection. People tend to use the phrase “variable selection” when the compet-\n",
            "ing models differ on which variables should be included, but agree on the mathemat-\n",
            "ical form that will be used for each variable — e.g., temperature might or might not\n",
            "be included as a predictor, but there is no question about whether, if it is, we’d use\n",
            "temperature or temperature2 or logtemperature.\n",
            "Since variable selection is a special case of model selection, and we’ve talked ex-\n",
            "tensively about model selection already (see especially Chapter 19), this chapter can\n",
            "Note to self: so why not merge be briefer than usual.\n",
            "this all into the model-selection\n",
            "chapter?\n",
            "22.2\n",
            "Why Variable Selection Using p-Values Is a Bad\n",
            "Idea\n",
            "When we assume the linear, constant-variance, independent-Gaussian-noise model is\n",
            "completely correct, it is easy to test the hypothesis that any particular coefﬁcient is\n",
            "zero. The (Wald) test statistic is\n",
            "ˆβi\n",
            "�se\n",
            "�\n",
            "ˆβi\n",
            "�\n",
            "and, under the null hypothesis that βi = 0, this has a tn−(p+1) distribution, therefore\n",
            "tending to a z (standard-Gaussian) distribution as n → ∞.\n",
            "It is very, very tempting, and common, to use the p-values which come from this\n",
            "test to select variables: signiﬁcant variables get included, insigniﬁcant ones do not,\n",
            "ones with smaller p-values (hence larger test statistics) are higher priorities to include\n",
            "than ones with smaller test statistics. This pattern of reasoning shows up over and over\n",
            "again among users of regression, including, I am ashamed to say, not a few statisticians.\n",
            "372\n",
            "\n",
            "\n",
            "Page 373:\n",
            "373\n",
            "22.2. WHY VARIABLE SELECTION USING P-VALUES IS A BAD IDEA\n",
            "The reasons why this is a bad idea were already gone over in Chapter 13, so, again,\n",
            "I will be brief. Let us think about what will tend to make the test statistic larger or\n",
            "smaller, by being more explicit about the denominator:\n",
            "ˆβi\n",
            "ˆσ\n",
            "�\n",
            "n�\n",
            "Var[Xi]\n",
            "�\n",
            "V I Fi\n",
            "where �\n",
            "Var[Xi] is the sample variance of the ith predictor variable, and V I Fi is that\n",
            "variables variance-inﬂation factor (see Chapter 15). What follows from this?\n",
            "1. Larger coefﬁcients will, all else being equal, have larger test statistics and be\n",
            "more signiﬁcant ( ˆβi in the numerator).\n",
            "2. Reducing the noise around the regression line will increase all the test statistics,\n",
            "and make every variable more signiﬁcant (ˆσ in the denominator).\n",
            "3. Increasing the sample size will increase all the test statistics, and make every\n",
            "variable more signiﬁcant (�n in the denominator).\n",
            "4. More variance in a predictor variable will, all else being equal, increase the test\n",
            "statistic and make the variable more signiﬁcant (�\n",
            "Var[Xi] in the denominator).\n",
            "5. More correlation between Xi and the other predictors will, all else being equal,\n",
            "decrease the test statistic and make the variable less signiﬁcant (V I Fi in the\n",
            "denominator).\n",
            "The test statistic, and thus the p-value, runs together an estimate of the actual size\n",
            "of the coefﬁcient with how well we can measure that particular coefﬁcient. This is\n",
            "exactly the right thing to do if our question is “Can we reliably detect that this coefﬁ-\n",
            "cient isn’t exactly zero?” That is a very, very different question from “Is this variable\n",
            "truly relevant to the response?”, or even from “Does including this variable help us\n",
            "predict the response?” Utterly trivial variables can show up as having highly signif-\n",
            "icant coefﬁcients, if the predictor has lots of variance and isn’t very correlated with\n",
            "the other predictors. Very important (large-coefﬁcient) variables can be insigniﬁcant,\n",
            "when their coefﬁcients can’t be measured precisely with our data. Every variable\n",
            "whose coefﬁcient isn’t exactly zero will eventually (as n → ∞) have an arbitrarily\n",
            "large test statistic, and an arbitrarily small p-value1\n",
            "None of this is even much help in answering the question “Which variables help\n",
            "us predict the response?”, let alone “Which variables help us explain the response?”\n",
            "None of this is ﬁxed by using F -tests on groups of coefﬁcients, rather than t-tests\n",
            "on individual coefﬁcients.\n",
            "1“Oh my God, it’s full of stars.” — David Bowman, on increasing his sample size to 2001.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 374:\n",
            "22.3. CROSS-VALIDATION INSTEAD\n",
            "374\n",
            "22.3\n",
            "Cross-Validation Instead\n",
            "If we want to use our models to make predictions, then what we want to know is\n",
            "how well the model will predict new data. The Cp statistic and AIC attempt to es-\n",
            "timate this, using how well the model predicted the old data, plus adjustments based\n",
            "on theory. Cross-validation estimates how well the model will predict new data by\n",
            "predicting new data. This is, unsurprisingly, a very good way of estimating how well\n",
            "the model will predict.\n",
            "The two main forms of cross-validation are leave-one-out, which we have already\n",
            "discussed in detail, and k-fold cross-validation, which we have spent less time on in\n",
            "class but is described in Chapter 19. They each have their strengths and weaknesses\n",
            "(which is why we have both).\n",
            "• k-fold CV is fast (the model gets ﬁt only k times, and typically k is 5 or 10);\n",
            "it is also “consistent for variable selection”, meaning that if one of the models\n",
            "presented to it contains all the relevant predictors, and only the relevant pre-\n",
            "dictors, then the probability of picking that right model → 1 as n → ∞. On\n",
            "the other hand, it tends to give somewhat worse predictions than leave-one-out,\n",
            "especially when all the models are wrong.\n",
            "• Leave-one-out can be slow (because the model must be ﬁt n times), except for\n",
            "linear regression where there is a short-cut formula. LOOCV is in-consistent\n",
            "for variable selection: even with unlimited amounts of data, it tends to include\n",
            "more variables than are necessary, though it will tend to include all the relevant\n",
            "variables. The model it picks tends to have lower prediction errors on new data\n",
            "than those picked by k-fold CV.\n",
            "As discussed in Chapter 19, Cp and AIC are best seen as approximations to leave-\n",
            "one-out, which avoid the step of re-ﬁtting the model, or even of calculating the short-\n",
            "cut formula (which still involves summing over every data point).\n",
            "22.4\n",
            "Stepwise Variable Selection\n",
            "“Stepwise” or “stagewise” variable selection is a family of methods for adding or re-\n",
            "moving variables from a model sequentially.\n",
            "Forward stepwise regression starts with a small model (perhaps just an intercept),\n",
            "considers all one-variable expansions of the model, and adds the variable which is\n",
            "best according to some criterion. This criterion might be “lowest p-value”, “high-\n",
            "est adjusted R2”, “lowest Mallow’s Cp”, “lowest AIC”, “lowest score under cross-\n",
            "validation”, etc. This process is then repeated, always adding one variable at a time,\n",
            "until the criterion stops improving. In backwards stepwise regression, we start on the\n",
            "contrary with the largest model we’re willing ton contemplate, and keep eliminating\n",
            "variables until we no longer improve. The obvious forward-backward or mixed step-\n",
            "wise variable selection procedure will contemplating both adding and removing one\n",
            "variable at each step, and take the best step.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 375:\n",
            "375\n",
            "22.4. STEPWISE VARIABLE SELECTION\n",
            "In a forward-backward algorithm we could easily add one variable, then add or\n",
            "remove another, and then remove the ﬁrst variable we’d added. This is because these\n",
            "stepwise algorithms only look at models which are close (one variable away from)\n",
            "the variable we started with2 In principle, we could just look at all possible linear\n",
            "models based on a given set of variables, and compute our criterion (adjusted R2, Cp,\n",
            "AIC, LOOCV, etc.) for each one of them; this is called all-subsets variable selection,\n",
            "because each model corresponds to a subset of the variables. With p variables there\n",
            "are 2p possible models, so all-subsets regression becomes, literally, exponentially more\n",
            "time-consuming with more variables; this is the only real justiﬁcation for the stepwise\n",
            "procedures.\n",
            "22.4.1\n",
            "Stepwise Selection in R\n",
            "The simplest function for stepwise model selection is the step function, which is\n",
            "built in to R. It can do forward or backward selection, or both, and you can specify\n",
            "both the smallest model to consider (so those variables are always included), and the\n",
            "largest. It can, however, only use AIC or BIC as the selection criteria.\n",
            "Here’s an example of how it works3, for the real estate data set from homework\n",
            "84.\n",
            "real.estate <- read.csv(\"http://www.stat.cmu.edu/~cshalizi/mreg/15/hw/08/real-estate.csv\")\n",
            "# Fit a 'kitchen sink' model But don't try to use the ID numbers as a\n",
            "# predictor variable!\n",
            "(realty.lm.all <- lm(Price ~ . - ID, data = real.estate))\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ . - ID, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "##\n",
            "(Intercept)\n",
            "Sqft\n",
            "Bedroom\n",
            "Bathroom\n",
            "##\n",
            "-2.390e+06\n",
            "1.075e+02\n",
            "-9.712e+03\n",
            "-1.067e+02\n",
            "## Airconditioning\n",
            "Garage\n",
            "Pool\n",
            "YearBuild\n",
            "##\n",
            "-1.222e+04\n",
            "1.732e+04\n",
            "1.249e+04\n",
            "1.279e+03\n",
            "##\n",
            "Quality\n",
            "Lot\n",
            "AdjHighway\n",
            "##\n",
            "-5.390e+04\n",
            "1.422e+00\n",
            "-2.717e+04\n",
            "step(realty.lm.all, direction = \"backward\", trace = 0)\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ Sqft + Bedroom + Garage + YearBuild + Quality +\n",
            "##\n",
            "Lot, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "2As search algorithms, they are “greedy”.\n",
            "3The trace argument controls how much step prints out as it tries various models. Larger values print\n",
            "out more information; the default, trace=1, is already a lot. Setting it to zero suppresses this. I urge you\n",
            "to re-run these examples with trace=1, but I doing so would substantially lengthen these notes.\n",
            "4But without any attempt at cleaning the data by removing outliers, etc.; this is just to illustrate the\n",
            "syntax, not as a full-scale data analysis.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 376:\n",
            "22.4. STEPWISE VARIABLE SELECTION\n",
            "376\n",
            "## (Intercept)\n",
            "Sqft\n",
            "Bedroom\n",
            "Garage\n",
            "YearBuild\n",
            "##\n",
            "-2.233e+06\n",
            "1.093e+02\n",
            "-1.007e+04\n",
            "1.665e+04\n",
            "1.191e+03\n",
            "##\n",
            "Quality\n",
            "Lot\n",
            "##\n",
            "-5.223e+04\n",
            "1.415e+00\n",
            "By comparison to the kitchen sink model, this drops bathrooms, air-conditioning,\n",
            "the pool, and adjacency to highways.\n",
            "Of course, we could start with a very simple model and expand:\n",
            "(realty.lm.minimal <- lm(Price ~ 1, data = real.estate))\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ 1, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "## (Intercept)\n",
            "##\n",
            "277894\n",
            "step(realty.lm.minimal, scope = list(upper = realty.lm.all, lower = realty.lm.minimal),\n",
            "direction = \"forward\", trace = 0)\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ Sqft + Quality + YearBuild + Lot + Garage +\n",
            "##\n",
            "Bedroom, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "## (Intercept)\n",
            "Sqft\n",
            "Quality\n",
            "YearBuild\n",
            "Lot\n",
            "##\n",
            "-2.233e+06\n",
            "1.093e+02\n",
            "-5.223e+04\n",
            "1.191e+03\n",
            "1.415e+00\n",
            "##\n",
            "Garage\n",
            "Bedroom\n",
            "##\n",
            "1.665e+04\n",
            "-1.007e+04\n",
            "This begins with an intercept-only model, and then adds variables. Here giving\n",
            "a lower limit to the scope is pretty much superﬂuous, we could just give it an upper\n",
            "limit, but it doesn’t hurt.\n",
            "Of course, we can also ask step to consider both adding and subtracting variables:\n",
            "step(realty.lm.minimal, scope = list(upper = realty.lm.all, lower = realty.lm.minimal),\n",
            "direction = \"both\", trace = 0)\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ Sqft + Quality + YearBuild + Lot + Garage +\n",
            "##\n",
            "Bedroom, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "## (Intercept)\n",
            "Sqft\n",
            "Quality\n",
            "YearBuild\n",
            "Lot\n",
            "##\n",
            "-2.233e+06\n",
            "1.093e+02\n",
            "-5.223e+04\n",
            "1.191e+03\n",
            "1.415e+00\n",
            "##\n",
            "Garage\n",
            "Bedroom\n",
            "##\n",
            "1.665e+04\n",
            "-1.007e+04\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 377:\n",
            "377\n",
            "22.5. INFERENCE AFTER SELECTION, AGAIN\n",
            "(This just so happens to reach the same answer as only doing forward selection.)\n",
            "If we want to only consider models which include certain terms, we can do that\n",
            "through changing the lower limit of the scope:\n",
            "(realty.lm.comforts <- lm(Price ~ Pool + Airconditioning, data = real.estate))\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ Pool + Airconditioning, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "##\n",
            "(Intercept)\n",
            "Pool\n",
            "Airconditioning\n",
            "##\n",
            "188852\n",
            "64335\n",
            "101760\n",
            "step(realty.lm.comforts, scope = list(upper = realty.lm.all, lower = realty.lm.comforts),\n",
            "direction = \"both\", trace = 0)\n",
            "##\n",
            "## Call:\n",
            "## lm(formula = Price ~ Pool + Airconditioning + Sqft + Quality +\n",
            "##\n",
            "YearBuild + Lot + Garage + Bedroom, data = real.estate)\n",
            "##\n",
            "## Coefficients:\n",
            "##\n",
            "(Intercept)\n",
            "Pool\n",
            "Airconditioning\n",
            "Sqft\n",
            "##\n",
            "-2.346e+06\n",
            "1.279e+04\n",
            "-1.176e+04\n",
            "1.080e+02\n",
            "##\n",
            "Quality\n",
            "YearBuild\n",
            "Lot\n",
            "Garage\n",
            "##\n",
            "-5.388e+04\n",
            "1.256e+03\n",
            "1.389e+00\n",
            "1.724e+04\n",
            "##\n",
            "Bedroom\n",
            "##\n",
            "-9.756e+03\n",
            "The step function is a simpliﬁed version of the function stepAIC in the MASS\n",
            "package, which works very similarly but is more ﬂexible. The leaps package contains\n",
            "an even more ﬂexible function, subsetreg, which tries to determine the lowest-MSE\n",
            "model at any given number of variables, and then lets you chose how to trade the\n",
            "number of parameters against MSE.\n",
            "22.5\n",
            "Inference after Selection, Again\n",
            "The standard inferential statistics (like the p-values on individual coefﬁcients) are only\n",
            "valid if the model is chosen independent of the data being used to calculate them. If\n",
            "there is any sort of data-dependent model selection, whether stepwise variable selec-\n",
            "tion or something else, they are no longer valid. This applies even to eliminating\n",
            "variables because their coefﬁcients are insigniﬁcant. If we do go ahead and use the\n",
            "same data twice, once to pick a model and once to test hypotheses about that model,\n",
            "we will get conﬁdence intervals which are systematically too narrow, p-values which\n",
            "are systematically too small, etc. (See Chapter 19 for more discussion, and an example\n",
            "of how doing model selection on pure noise can lead to apparently highly-signiﬁcant\n",
            "results.)\n",
            "The easy cure, as discussed in Chapter 19, is to split the data in half at random,\n",
            "and use one part to do model selection and the other half to do inference for your\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 378:\n",
            "22.6. FURTHER READING\n",
            "378\n",
            "selected model. Again, there is nothing about variable selection which makes this\n",
            "any different.\n",
            "22.6\n",
            "Further Reading\n",
            "In general, all the references for Chapter 19 are relevant again.\n",
            "For a vivid example of just how badly misleading selecting variables based on sta-\n",
            "tistical signiﬁcance can be, see Ward et al. (2010).\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 379:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Chapter 23\n",
            "Trees\n",
            "This lecture was based entirely on the corresponding chapter of Shalizi (forthcoming).\n",
            "I should either write a new chapter, or omit.\n",
            "379\n",
            "\n",
            "\n",
            "Page 380:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Chapter 24\n",
            "The Bootstrap I\n",
            "24.1\n",
            "Statistical Inference, Assuming Gaussian Noise\n",
            "Consider our usual linear model\n",
            "Y = xβ + ε\n",
            "For a lot of results, it’s enough to assume that\n",
            "�[ε|x] = 0\n",
            "and\n",
            "Var[ε|x] = σ2I\n",
            "These assumptions are enough to show the consistency of the least squares estimate\n",
            "�\n",
            "β = (xT x)−1xT y\n",
            "The reason is that these assumptions let us write\n",
            "�\n",
            "β = β + (xT x)−1xT ε ,\n",
            "i.e., they let us write the estimate as “true value plus weighted sum of the noise terms”.\n",
            "Just as with simple linear regression, we can use this trick to get at a lot of properties\n",
            "of �\n",
            "β: we can show that it’s unbiased; that it has variance matrix σ2(xT x)−1; that,\n",
            "from the previous two properties, �\n",
            "β → β as n → ∞. But these assumptions are not\n",
            "enough to get useful hypothesis tests or conﬁdence intervals.\n",
            "For those, we have, so far, assumed that\n",
            "ε ∼ N(0,σ2I)\n",
            "independent of x. This Gaussian noise assumption is important, because it gives us\n",
            "the distribution of �\n",
            "β:\n",
            "�\n",
            "β ∼ N(β,σ2(xT x)−1)\n",
            "380\n",
            "\n",
            "\n",
            "Page 381:\n",
            "381\n",
            "24.1. STATISTICAL INFERENCE, ASSUMING GAUSSIAN NOISE\n",
            "And the reason for that is that if ε is Gaussian, then (xT x)−1xT ε is a linear transfor-\n",
            "mation of a Gaussian, which is also Gaussian. From knowing that �\n",
            "β is a Gaussian,\n",
            "everything we’ve done by way of statistical inference follows: the hypothesis tests,\n",
            "the conﬁdence intervals, the prediction intervals, the F tests for multiple coefﬁcients,\n",
            "etc. Without Gaussian noise, few of the formulas you memorized for the exam are,\n",
            "strictly, correct.\n",
            "Looking at Q−Q plots of our residuals is only important because we want them to\n",
            "be Gaussian. The only justiﬁcation for ever contemplating a Box-Cox transformation\n",
            "is that we’d like the noise to be Gaussian. We really have little reason to ever expect\n",
            "Gaussian noise; it’s just very useful when it happens.\n",
            "24.1.1\n",
            "Other Parametric Distributions of the Noise\n",
            "Suppose we didn’t know think that ε was Gaussian, but still believed it was indepen-\n",
            "dently and identically distributed (IID). We might, for instance, think it had a “double\n",
            "exponential” (or “Laplacian”) distribution, with probability density function\n",
            "f (ε) ∝ e−|ε|/L ,\n",
            "or was a scaled t distribution with a certain number ν of degrees of freedom,\n",
            "f (ε) ∝\n",
            "�\n",
            "1 + (x/s)2\n",
            "ν\n",
            "�−(ν+1)/2\n",
            "Both of these have heavier tails than the Gaussian distribution, so they can be very\n",
            "useful in practice.\n",
            "If we think that the deterministic part of the model should still be linear, we still\n",
            "use least squares to get the estimate �\n",
            "β, and it’s still true that\n",
            "�\n",
            "β = β + (xT x)−1xT ε\n",
            "(24.1)\n",
            "But a linear combination of double-exponential variables is just a mess, as is a linear\n",
            "combination of t-distributions. A family of distributions where adding two random\n",
            "variables gives another distribution in the same family is called stable. The Gaussian\n",
            "distributions are stable, but most others aren’t, and hence trying to work out an exact\n",
            "sampling theory for most other noise distributions, like the one we have for Gaussian\n",
            "noise, is pretty hopeless.\n",
            "24.1.2\n",
            "Asymptotic Gaussianity\n",
            "Still, let’s think about the noisy part of Eq. 24.1 some more. It’s\n",
            "(xT x)−1xT ε\n",
            "Let’s bundle up (xT x)−1xT as a matrix, call it k. Then\n",
            "ˆβi = βi +\n",
            "n\n",
            "�\n",
            "j=1\n",
            "ki jεj\n",
            "(24.2)\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 382:\n",
            "24.1. STATISTICAL INFERENCE, ASSUMING GAUSSIAN NOISE\n",
            "382\n",
            "If all of the ki j were equal, we’d understand what was going on. After all, the central\n",
            "limit theorem says that when εj are IID with mean 0 and variance σ2, their average\n",
            "tends towards a Gaussian distribution:\n",
            "n\n",
            "�\n",
            "j=1\n",
            "1\n",
            "n εj ⇝ N(0,σ2/n)\n",
            "This is true whatever the distribution of the εj might be, provided, to repeat, that\n",
            "they’re IID and they have mean 0 and variance 0 < σ2 < ∞. If instead of multiplying\n",
            "each εj by 1/n we multiplied them by some other constant, we’d change the variance\n",
            "but still tend towards a Gaussian:\n",
            "n\n",
            "�\n",
            "j=1\n",
            "kεj ⇝ N(0,σ2k2n)\n",
            "On this basis, we might hope that, as n → ∞,\n",
            "n\n",
            "�\n",
            "j=1\n",
            "ki jεj ⇝ N(0,σ2 �\n",
            "j\n",
            "k2\n",
            "i j)\n",
            "The difﬁculty is that the terms in the sum, while still statistically independent, are no\n",
            "longer identically distributed. There are central limit theorems which apply to inde-\n",
            "pendent, non-identically distributed random variables, with the basic result being that\n",
            "if none of the ki j is too big compared to the others, the sum is indeed asymptotically\n",
            "Gaussian1.\n",
            "If the matrix x doesn’t give too much inﬂuence to any particular observations,\n",
            "then these central limit theorems usually apply, and we can say that\n",
            "�\n",
            "β ⇝ N(β,σ2(xT x)−1)\n",
            "(24.3)\n",
            "as n → ∞, if ε is non-Gaussian but IID. From there, of course, all the usual formulas\n",
            "would also come to hold as n → ∞.\n",
            "How close to inﬁnity does n have to be? You may have been told a bit of folklore\n",
            "which says that the central limit theorem dominates the behavior of a sample mean\n",
            "once n > 30. This is badly wrong even for averages, let alone more complicated\n",
            "functions like regression estimates. There is really no upper limit on how big n might\n",
            "have to be before Eq. 24.3 becomes a good approximation.\n",
            "24.1.3\n",
            "Summing Up on Gaussian Noise\n",
            "To sum up, we have two situations:\n",
            "1. We can assume that the noise is exactly Gaussian and independent, and have a\n",
            "nice body of theory for statistical inference, but we hardly ever see that happen.\n",
            "1If you want to follow this up, this is the “Lindeberg” central limit theorem.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 383:\n",
            "383\n",
            "24.2. THE SAMPLING DISTRIBUTION AS THE SOURCE OF ALL\n",
            "KNOWLEDGE ABOUT UNCERTAINTY\n",
            "2. We can assume that the noise is just independent, and recover the Gaussian\n",
            "theory asymptotically as n → ∞, but we don’t know what to do when n is\n",
            "ﬁnite, or even how big n has to get.\n",
            "Clearly, this is an unsatisfying situation.\n",
            "24.2\n",
            "The Sampling Distribution as the Source of All\n",
            "Knowledge about Uncertainty\n",
            "Our data comes from some distribution, let’s say P. We would like to know some\n",
            "property of this distribution, say θ. (We may think of this as a regression coefﬁcient,\n",
            "or the whole coefﬁcient vector, or �[Y |X = x] for a particular x, etc.) Since we do\n",
            "not know P, we can’t just calculate θ. What we can do, however, is draw a sample D\n",
            "from P, and then we calculate some statistic or other, T (D). This serves as our esti-\n",
            "mate of θ. (The same goes for hypothesis tests, etc.) Because the data D are random,\n",
            "so is T . In fact, the distribution of T , the sampling distribution of our statistic, is set\n",
            "by the distribution of D. If we knew the sampling distribution, we’d know basically\n",
            "everything there is to know for statistical inference:\n",
            "• The bias would be �[T ] − θ\n",
            "• The standard error would\n",
            "�\n",
            "Var[T ]\n",
            "• Hypothesis tests would come from quantiles of T\n",
            "• Conﬁdence intervals would come from inverting hypothesis tests\n",
            "Unfortunately,\n",
            "• Interesting statistics T are very complicated functions of the data, so their sam-\n",
            "pling distribution is complicated even if P is simple;\n",
            "• Realistic distributions P are usually also complicated; and\n",
            "• We don’t know P anyway.\n",
            "Put these together, and we should be surprised we can ever get nice, useful formulas\n",
            "for the sampling distribution of any statistic, rather than disappointed that we can’t\n",
            "make it happen for regression without Gaussian noise.\n",
            "The reason we can work out the sampling distributions for regression with Gaus-\n",
            "sian noise is that we (or rather, the ancestors) carefully adjusted the assumptions about\n",
            "the noise, the model, and the statistic just so, and everything came together. (E.g., we\n",
            "needed both a linear estimator and a noise distribution which was stable under linear\n",
            "combinations.) What could we do if we can’t, or won’t, ﬁne-tune all the assumptions?\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 384:\n",
            "24.3. THE MONTE CARLO PRINCIPLE\n",
            "384\n",
            "24.3\n",
            "The Monte Carlo Principle\n",
            "What is sometimes called the Monte Carlo principle is a general strategy for ﬁguring\n",
            "out the behavior of complicated functions of complicated distributions: it says to\n",
            "simulate it and see what happens2. If we have samples D1,D2,...Db from P, and we\n",
            "want to know the expectation of T (D), we can approximate that as\n",
            "�[T (D)] ≈ 1\n",
            "b\n",
            "b\n",
            "�\n",
            "i=1\n",
            "T (Di) ≡ T\n",
            "If we want to know the variance, we can approximate that by\n",
            "Var[T (D)] ≈ 1\n",
            "b\n",
            "b\n",
            "�\n",
            "i=1\n",
            "(T (Di) − T )2\n",
            "If we want to know the qth quantile of T , we can order the T (Di) from smallest\n",
            "to largest, and take the qb value as our estimate. If we want an interval which will\n",
            "contain T with probability 1 − α, we order the T (Di) from smallest to largest, and\n",
            "exclude those with rank from 1 to bα/2 on one side, and from b(1 − α/2) to b on\n",
            "the other. And so on and so forth. All we need to do to get at any property of any\n",
            "function of the distribution P is to be able to draw from, or simulate, P.\n",
            "One limitation of this for statistics is that, of course, we don’t know the true P.\n",
            "24.4\n",
            "The Bootstrap Principle\n",
            "The bootstrap principle is that if we have good approximation ˆP to P, we can sim-\n",
            "ulate from ˆP, and get a good approximation to the sampling distribution we want.\n",
            "That is, we apply the Monte Carlo principle to a distribution ( ˆP) which we hope is\n",
            "close to the distribution we really care about (P).\n",
            "More speciﬁcally, bootstrapping is always an algorithm, which goes, abstractly, as\n",
            "follows:\n",
            "1. Observe data D, calculate estimate T (D) and get an approximation ˆP to P\n",
            "2. Repeat b times:\n",
            "(a) Simulate surrogate data ˜D from ˆP.\n",
            "(b) Calculate ˜T = T ( ˜D), just as those ˜D were real data\n",
            "3. Approximate the distribution of T under P with the distribution of ˜T under\n",
            "ˆP\n",
            "2The name, and to some extent the technique, originated with the physicists designing ﬁrst the atomic\n",
            "and then the hydrogen bomb. Those designs required calculating the expectations of many elaborate func-\n",
            "tions of complex distributions (Serber, 1992). Rather than trying to actually do the integrals involved, they\n",
            "just developed efﬁcient ways to sample from the distributions, and computed sample averages (Metropolis\n",
            "et al., 1953). The spirit of “try it, and see what happens” went deep at Los Alamos...\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 385:\n",
            "385\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "There are a lot of variants and reﬁnements, some of which we will cover as this\n",
            "goes on, but in the meanwhile, we really need to be clearer about what “a good ap-\n",
            "proximation ˆP to P” might mean.\n",
            "The two basic options are model-based bootstraps and re-sampling bootstraps3.\n",
            "In a model-based bootstrap, our approximation ˆP is a full model of the data gener-\n",
            "ating process, which we’ve estimated; we then simulate that model. In a re-sampling\n",
            "bootstrap, we treat the sample we observed as our best estimate at the distribution\n",
            "of the whole population, and so we draw a new sample from our original sample —\n",
            "we re-sample it. (If you like, in re-sampling the empirical distribution is our model.)\n",
            "Both of these ideas can be made a bit more concrete in the context of regression.\n",
            "24.5\n",
            "Bootstraps for Regression\n",
            "Any regression model can be written as\n",
            "Y = m(X) + ε\n",
            "with the caveat that the noise term ε might not have expectation zero4 or be indepen-\n",
            "dent of X. Specifying the true regression function m and the distribution of the noise\n",
            "ε, including its dependence on X, gives us the data-generating distribution P.\n",
            "Depending on what we are willing to believe about the true regression function\n",
            "m and the noise ε, we have different ways of coming up with approximations ˆP to P,\n",
            "and different ways of simulating from those approximations.\n",
            "24.5.1\n",
            "The Linear, Gaussian Bootstrap\n",
            "The simplest case we could have is where we think all of our usual modeling assump-\n",
            "tions hold, so that m(x) = xβ and ε ∼ N(0,σ2), IID and independent of x. Then\n",
            "simulating from the estimated model is very easy.\n",
            "# Simulate from a previously fitted linear model with Gaussian noise Inputs:\n",
            "# model; data frame Outputs: new data frame with response values replaced\n",
            "# Presumes: all necessary variables are in data frame\n",
            "sim.lm.gauss <- function(mdl, df) {\n",
            "# What's the response variable called?\n",
            "Should be the first variable in the\n",
            "# vector of all variables\n",
            "resp.var <- all.vars(formula(mdl))[1]\n",
            "# What value should we expect for the response?\n",
            "expect.resp <- predict(mdl, newdata = df)\n",
            "# How big is the noise?\n",
            "sigma2.mle <- mean(residuals(mdl)^2)\n",
            "# Add appropriately-sized Gaussian noise to the response\n",
            "response <- expect.resp + rnorm(nrow(df), 0, sqrt(sigma2.mle))\n",
            "3Often called these “parametric” and “non-parametric”, respectively, but that’s not quite as transparent,\n",
            "I think, as the other names.\n",
            "4For instance, if the m(X) is biased.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 386:\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "386\n",
            "df[, resp.var] <- response\n",
            "# Won't change df outside this function!\n",
            "return(df)\n",
            "}\n",
            "What we are doing in this function is creating a (very small!) imaginary or alterna-\n",
            "tive world, the simulation world or bootstrap world, where we know that the model\n",
            "Y = x ˆβ + ε, ε ∼ N(0, ˆσ2) is exactly true. If ˆβ ≈ β and ˆσ2 ≈ σ2, and ε is Gaussian,\n",
            "then what we see in the simulation world is (close to) representative of what would\n",
            "happen in the real world if we could repeat our experiments many times. The ad-\n",
            "vantage of the simulation world is that it’s easy to re-run the simulation many times,\n",
            "whereas repeating the experiment may be expensive, difﬁcult, unethical or ﬂat-out\n",
            "impossible.\n",
            "Of course, we don’t just want to get a new data set, from an new simulation-world\n",
            "experiment; we want to know what we’d have concluded from that experiment. This\n",
            "is also easy.\n",
            "# Re-estimate a linear model on a new data set Inputs: old model; data frame\n",
            "# Output: new lm object Presumes: data frame contains columns with\n",
            "# appropriate names\n",
            "re.lm <- function(mdl, df) {\n",
            "return(lm(formula(mdl), data = df))\n",
            "}\n",
            "Now if we want to get at the sampling distribution of, say, the estimated coefﬁcient\n",
            "vector �\n",
            "β, we just simulate it. We’ll need some particular initial estimate to work with,\n",
            "so let’s try to predict how much a cat’s heart will weigh, from the cat’s total body\n",
            "weight and its sex:\n",
            "library(MASS)\n",
            "data(cats)\n",
            "cats.lm <- lm(Hwt ~ Sex * Bwt, data = cats)\n",
            "We now simulate from our cats.lm model many times (10000 is a conveniently\n",
            "small number), re-estimate the coefﬁcients each time, and store the re-estimates in an\n",
            "array:\n",
            "beta.boots <- replicate(10000, coefficients(re.lm(cats.lm, sim.lm.gauss(cats.lm,\n",
            "cats))))\n",
            "beta.boots is now a 4,10000 array, with one row for each coefﬁcient, and 10000\n",
            "columns, because we replicated the simulation 10000 times. Each column is a separate\n",
            "visit to the bootstrap world, where the true value of β is ﬁxed to our initial estimate\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 387:\n",
            "387\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "�\n",
            "β. Symbolically, it looks like\n",
            "\n",
            "\n",
            "˜β01\n",
            "˜β02\n",
            "...\n",
            "˜β0b\n",
            "˜β11\n",
            "˜β12\n",
            "...\n",
            "˜β1b\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "˜βp1\n",
            "˜βp2\n",
            "...\n",
            "˜βpb\n",
            "\n",
            "\n",
            "That being the case, we can get the bias:\n",
            "rowMeans(beta.boots) - coefficients(cats.lm)\n",
            "##\n",
            "(Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "##\n",
            "0.0021254228 -0.0340159618 -0.0007264433\n",
            "0.0119192950\n",
            "Each row contains all of our samples for one coefﬁcient estimate, so the mean\n",
            "along each row is our (approximate) expected value of the estimate; we subtract the\n",
            "truth from that to get the bias.\n",
            "We can also get the standard errors:\n",
            "apply(beta.boots, 1, sd)\n",
            "## (Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "##\n",
            "1.8338772\n",
            "2.0395854\n",
            "0.7709635\n",
            "0.8275103\n",
            "There is no rowSDs function, but the utility (or meta-) function apply lets us take\n",
            "any array (the ﬁrst argument) and apply any function (the third argument) to either\n",
            "all its rows (middle argument 1) or all its columns (middle argument 2), or every entry\n",
            "in the array (middle argument c(1,2)). So the incantation above takes the standard\n",
            "deviation of each row.\n",
            "To get a 1−α conﬁdence interval, we (conceptually) take all the values we got for\n",
            "each coefﬁcient, sort them, and discard the lower and upper α/2 tails. The appropriate\n",
            "incantation for 95% intervals is\n",
            "apply(beta.boots, 1, quantile, prob = c(0.05/2, 1 - 0.05/2))\n",
            "##\n",
            "(Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "## 2.5%\n",
            "-0.5570232 -8.2212841 1.123800 0.09212372\n",
            "## 97.5%\n",
            "6.5648374 -0.2623286 4.130409 3.33158904\n",
            "Now, none of this is actually necessary if we assume the truth is linear-and-Gaussian.\n",
            "We know that the bias is zero; we know that the standard deviations come from\n",
            "ˆσ2(xT x)−1; speciﬁcally, for the cats, they’re\n",
            "coefficients(summary(cats.lm))[, \"Std. Error\"]\n",
            "## (Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "##\n",
            "1.8428394\n",
            "2.0617552\n",
            "0.7759022\n",
            "0.8373255\n",
            "We also know how to calculate the conﬁdence intervals:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 388:\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "388\n",
            "confint(cats.lm, level = 0.95)\n",
            "##\n",
            "2.5 %\n",
            "97.5 %\n",
            "## (Intercept) -0.6620801\n",
            "6.62470490\n",
            "## SexM\n",
            "-8.2416012 -0.08919944\n",
            "## Bwt\n",
            "1.1024137\n",
            "4.17041438\n",
            "## SexM:Bwt\n",
            "0.0208271\n",
            "3.33170228\n",
            "The fact that these numbers are very close to what we got by simulation tells us\n",
            "two things:\n",
            "1. Simulation can work to replace intricate probabilistic mathematics with straight-\n",
            "forward (even simple-minded) computations.\n",
            "2. Simulation is completely redundant in the linear-Gaussian case, where we, and\n",
            "R, know all the formulas.\n",
            "Simulation comes into its own when the formulas aren’t available.\n",
            "24.5.2\n",
            "Linear, Non-Gaussian Noise\n",
            "Suppose that we think the residuals follow some particular non-Gaussian distribution,\n",
            "which we know up to some set of parameters, e.g., a t distribution. (We might have\n",
            "reached this conviction either because of some actual scientiﬁc theory, or by staring\n",
            "at the plot of the residuals.) If we know how to estimate the parameters of this noise\n",
            "distribution, and we can simulate from it, then we are in business.\n",
            "I will illustrate this idea by using a t distribution for the noise in the model of\n",
            "cat’s hearts. This is not an especially great model for this data, but it’s only meant as\n",
            "an illustration5. We’ll need to estimate the parameters of the t distribution; this job\n",
            "is already done for us by the function fitdistr in the MASS library.\n",
            "# Simulate from a previously fitted linear model with t-distributed noise\n",
            "# Inputs: model; data frame Outputs: new data frame with response values\n",
            "# replaced Presumes: all necessary variables are in data frame\n",
            "sim.lm.t <- function(mdl, df) {\n",
            "# What's the response variable called?\n",
            "resp.var <- all.vars(formula(mdl))[1]\n",
            "# What value should we expect for the response?\n",
            "expect.resp <- predict(mdl, newdata = df)\n",
            "# Estimate the t parameters, using MASS::fitdistr\n",
            "stopifnot(require(MASS))\n",
            "# Make sure the library's available\n",
            "# After the example in help(fitdistr)\n",
            "mydt <- function(x, s, df) {\n",
            "dt(x/s, df)/s\n",
            "}\n",
            "t.params <- fitdistr(residuals(cats.lm), mydt, start = list(s = 1, df = 50),\n",
            "lower = c(0, 1))$estimate\n",
            "# Add appropriately-sized t-noise to the response\n",
            "5t-distributed noise is a much better idea in areas like ﬁnance.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 389:\n",
            "389\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "response <- expect.resp + t.params[\"s\"] * rt(nrow(df), df = t.params[\"df\"])\n",
            "df[, resp.var] <- response\n",
            "# Won't change df outside this function\n",
            "return(df)\n",
            "}\n",
            "Having changed the function we use to simulate, absolutely nothing else needs to\n",
            "change:\n",
            "beta.boots2 <- replicate(10000, coefficients(re.lm(cats.lm, sim.lm.t(cats.lm,\n",
            "cats))))\n",
            "apply(beta.boots2, 1, quantile, prob = c(0.05/2, 1 - 0.05/2))\n",
            "##\n",
            "(Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "## 2.5%\n",
            "-0.6042652 -8.1174195 1.122781 0.07208883\n",
            "## 97.5%\n",
            "6.5887639 -0.1965905 4.119302 3.27106954\n",
            "24.5.3\n",
            "Resampling Residuals\n",
            "Suppose we are willing to believe that\n",
            "Y = xβ + ε\n",
            "and even that ε is independent of x, but not that we have any good idea about what\n",
            "the distribution of ε is. What are we to do?\n",
            "Well, it will still be true that our residuals will give us an estimate of what the\n",
            "noise ε looks like — of what the true noise distribution is. We can use that. When\n",
            "we generate new data, we’ll take x �\n",
            "β + ˜ε, where ˜ε is our simulated noise. Every time\n",
            "we need a value for ˜ε, we’ll go to our vector of residuals and draw a random value\n",
            "from it — we will re-sample the residuals, with replacement. Here’s how it works\n",
            "computationally:\n",
            "# Simulate from a previously fitted linear model, resampling residuals\n",
            "# Inputs: model; data frame Outputs: new data frame with response values\n",
            "# replaced Presumes: all necessary variables are in data frame\n",
            "sim.lm.residuals <- function(mdl, df) {\n",
            "# What's the response variable called?\n",
            "resp.var <- all.vars(formula(mdl))[1]\n",
            "# What value should we expect for the response?\n",
            "expect.resp <- predict(mdl, newdata = df)\n",
            "# Resample the residuals\n",
            "new.noise <- sample(residuals(mdl), size = length(expect.resp), replace = TRUE)\n",
            "# Add new noise to the expected response\n",
            "response <- expect.resp + new.noise\n",
            "df[, resp.var] <- response\n",
            "# Won't change df outside this function\n",
            "return(df)\n",
            "}\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 390:\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "390\n",
            "When (as is usually the case) we want the re-sample to be exactly as large as the\n",
            "original sample, we can deﬁne a little convenience function:\n",
            "resample <- function(x) {\n",
            "sample(x, size = length(x), replace = TRUE)\n",
            "}\n",
            "(Note that, as written, this only works for vectors.) Let’s see what kind of things\n",
            "this does to a short vector:\n",
            "head(residuals(cats.lm))\n",
            "##\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "## -1.2541405 -0.8541405\n",
            "1.2458595 -1.3177819 -1.2177819 -0.9177819\n",
            "resample(head(residuals(cats.lm)))\n",
            "##\n",
            "1\n",
            "2\n",
            "2\n",
            "6\n",
            "1\n",
            "5\n",
            "## -1.2541405 -0.8541405 -0.8541405 -0.9177819 -1.2541405 -1.2177819\n",
            "resample(head(residuals(cats.lm)))\n",
            "##\n",
            "3\n",
            "3\n",
            "3\n",
            "6\n",
            "5\n",
            "4\n",
            "##\n",
            "1.2458595\n",
            "1.2458595\n",
            "1.2458595 -0.9177819 -1.2177819 -1.3177819\n",
            "Every time we run this function, we get different results; some samples get picked\n",
            "more than once, some don’t get picked at all. When we look at the over-all distribu-\n",
            "tion of each re-sample, it’s somewhat less diverse than the sample we took it from, just\n",
            "as the sample is less diverse than the population, but it has the same over-all shape.\n",
            "par(mfrow = c(1, 3))\n",
            "hist(residuals(cats.lm), main = \"\", xlab = \"Residuals\")\n",
            "hist(resample(residuals(cats.lm)), main = \"\", xlab = \"Residuals\")\n",
            "hist(resample(residuals(cats.lm)), main = \"\", xlab = \"Residuals\")\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 391:\n",
            "391\n",
            "24.5. BOOTSTRAPS FOR REGRESSION\n",
            "Residuals\n",
            "Frequency\n",
            "−4\n",
            "−2\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "Residuals\n",
            "Frequency\n",
            "−4\n",
            "−2\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "Residuals\n",
            "Frequency\n",
            "−4\n",
            "−2\n",
            "0\n",
            "2\n",
            "4\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "With this understood, once we have our simulator for resampling residuals, we\n",
            "have to make absolutely no changes to how we use it:\n",
            "beta.boots3 <- replicate(10000, coefficients(re.lm(cats.lm, sim.lm.residuals(cats.lm,\n",
            "cats))))\n",
            "apply(beta.boots3, 1, quantile, prob = c(0.05/2, 1 - 0.05/2))\n",
            "##\n",
            "(Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "## 2.5%\n",
            "-0.6348473 -8.1095238 1.159932 0.05006452\n",
            "## 97.5%\n",
            "6.4986221 -0.1712509 4.156326 3.27066773\n",
            "24.5.4\n",
            "Resampling Cases\n",
            "The last bootstrap we’ll look at, here, is what’s variously called the case (or cases)\n",
            "bootstrap, or the pairs bootstrap, or, more rarely, the rows bootstrap. The idea here\n",
            "is that our data consists of cases, each of which contains the predictor variables and\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 392:\n",
            "24.6. ERROR IN THE BOOTSTRAP, AND WHICH BOOTSTRAP WHEN?392\n",
            "the response variables, and we’ll re-sample those whole points. The other names arise\n",
            "from thinking of each data point as a pair (x,y), or as a row in a data frame. We’re\n",
            "using as our approximation ˆP to the data-generating distribution P the joint empirical\n",
            "distribution of the predictors and the response.\n",
            "The simulator is now sheer elegance in its simplicity:\n",
            "# Re-sample the rows of a data frame Inputs: the data frame Output: a new\n",
            "# data frame, contain a random sample, with replacement, of rows from the\n",
            "# input\n",
            "resample.data.frame <- function(df) {\n",
            "df[resample(1:nrow(df)), ]\n",
            "}\n",
            "Notice that we don’t use the estimated model here at all in the simulation — the\n",
            "procedure is quite agnostic as to whether our model is good or bad.\n",
            "Having the simulator in hand, we can use it just like the others, and do inference\n",
            "using the simulation just like the others:\n",
            "beta.boots4 <- replicate(10000, coefficients(re.lm(cats.lm, resample.data.frame(cats))))\n",
            "apply(beta.boots4, 1, quantile, prob = c(0.05/2, 1 - 0.05/2))\n",
            "##\n",
            "(Intercept)\n",
            "SexM\n",
            "Bwt\n",
            "SexM:Bwt\n",
            "## 2.5%\n",
            "0.2694597 -7.7599948 1.481333 0.2882054\n",
            "## 97.5%\n",
            "5.7531055 -0.6688548 3.771389 3.0845020\n",
            "Resampling cases makes only very weak assumptions about the data-generating\n",
            "distribution, that all data points ((x,y) pairs) are independent and identically dis-\n",
            "tributed. It does not assume that any linear regression is correct6, or that the noise is\n",
            "independent of x, or has constant variance.\n",
            "24.6\n",
            "Error in the Bootstrap, and Which Bootstrap When?\n",
            "The bootstrap, remember, is a way of calculating the sampling distribution of T , un-\n",
            "der the true data-generating distribution P. There are two main sources of error in\n",
            "this calculation:\n",
            "Simulation We’d like to see the full distribution of our statistic T under ˆP, but instead we\n",
            "only run b simulations under ˆP.\n",
            "Approximation We’re simulating from ˆP rather than P.\n",
            "Simulation error (or “Monte Carlo error”) is easy to grasp, and in principle easy\n",
            "to control: the more simulations we run, the better. As b → ∞, the simulation goes\n",
            "to zero. The only reason to ever restrict b is that each simulation does have some\n",
            "6If the linear model is wrong, then we’re doing statistical inference on the coefﬁcients in the best linear\n",
            "approximation to the true regression function m(x).\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 393:\n",
            "39324.6. ERROR IN THE BOOTSTRAP, AND WHICH BOOTSTRAP WHEN?\n",
            "cost, in time if nothing else. In fact, for (most) inferential statistics, we can expect\n",
            "the simulation error to be O(1/\n",
            "�\n",
            "b), so if we keep increasing b we will experience\n",
            "diminishing returns, but never negative returns.\n",
            "Approximation error comes from the fact that ˆP is not P. This can itself be bro-\n",
            "ken into two parts: estimation error (roughly, variance), and systematic distortion\n",
            "(roughly, bias). Estimation error arises because we only have a ﬁnite amount of data,\n",
            "say n observations, with which to estimate ˆP. Even if we resample cases, and so use\n",
            "the empirical distribution as our ˆP, we still only have n samples from the full popula-\n",
            "tion, which isn’t all of it. Generally, estimation error will shrink to zero as n → ∞,\n",
            "but it may shrink at different rates for different approximations. Systematic distor-\n",
            "tion is basically the approximation error which would be left even if we had inﬁnite\n",
            "data — it comes from using a linear-Gaussian simulation when reality isn’t linear or\n",
            "Gaussian, or resampling residuals when the noise is really heteroskedastic.\n",
            "There is a trade-off when it comes to the two kinds of approximation error. The\n",
            "more we constrain ˆP in advance of seeing any data, the stronger the assumption we\n",
            "put on it, the less we have to estimate, and so the smaller the estimation error. But, the\n",
            "true P doesn’t obey those constraints, if our assumptions are wrong, the bigger the\n",
            "systematic distortion we’re introducing. If our assumptions are right, using a more\n",
            "constrained ˆP is pure advantage — basically, we’re not wasting data ﬁguring out that\n",
            "the constraints hold — but if those assumptions are wrong, they can easily make things\n",
            "worse.\n",
            "Which bootstrap to use, then, depends on how strongly you trust your modeling\n",
            "assumptions.\n",
            "• If you believe that the regression is linear and you know the distribution of the\n",
            "noise, use the fully model-based bootstraps.\n",
            "• If you believe that the regression is linear and the noise is independent of x, use\n",
            "resampling of residuals.\n",
            "• If you are unwilling to believe that the noise is independent of x, and/or that\n",
            "the regression is truly linear, use resampling of cases.\n",
            "(We’ll cover the situation where you don’t think the truth is linear but you are, some-\n",
            "how, convinced the noise is Gaussian when we go over ﬁtting nonlinear models in\n",
            "402.)\n",
            "How do we tell? Well, in the ﬁrst situation, all of the diagnostics we’ve been\n",
            "doing should look good, including the appropriate Q-Q plot. In the second situation,\n",
            "while the residuals should have the same distribution for all x, we don’t care what\n",
            "that distribution is. Therefore when we plot residuals against predictors and ﬁtted\n",
            "values, everything should look random, but not necessarily Gaussian, and the Q-Q\n",
            "plot need show no particular shape. In the third situation, by resampling cases we’re\n",
            "still assuming independence across data points, so we should try to check that, but\n",
            "that’s about all that we do need to check.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 394:\n",
            "24.7. FURTHER READING\n",
            "394\n",
            "24.7\n",
            "Further Reading\n",
            "The bootstrap was introduced, by that name, by Efron (1979), in a remarkably acces-\n",
            "sible paper which is still worth reading. Related ideas, such as the “jackknife” (omit\n",
            "one data point, re-estimate, and look at the variance over all such re-estimates) go back\n",
            "at least to the 1940s, though they weren’t systematically developed, or applied, until\n",
            "computing power got cheap enough to make something like the bootstrap feasible. A\n",
            "good systematic textbook is Davison and Hinkley (1997).\n",
            "For the validity of case resampling even when all the usual linear-Gaussian assump-\n",
            "tions fail, see, e.g., Buja et al. (2014). (That paper also shows how this bootstrap does\n",
            "the same job as the “robust standard errors” of econometrics.)\n",
            "Next time, we will look at some of the techniques for reducing approximation er-\n",
            "ror in bootstrap calculations, bootstrap prediction intervals, and what sorts of things\n",
            "the bootstrap can’t do; all of these are also covered in Davison and Hinkley (1997).\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 395:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Chapter 25\n",
            "The Bootstrap, II\n",
            "Note to self: currently a bit of\n",
            "a stub; merge this into previous\n",
            "chapter?\n",
            "25.1\n",
            "Improving the Bootstrap: Pivoting\n",
            "In Chapter ??, I said there were three causes of error when we use the bootstrap to do\n",
            "statistical inference, e.g., to calculate a conﬁdence interval. These were (1) using only\n",
            "a ﬁnite number of simulations; (2) simulating from an approximate distribution ˆP\n",
            "which systematically distorts the true, data-generating distribution P; and (3) having\n",
            "only a ﬁnite amount of data with which to estimate ˆP.\n",
            "Increasing the number of simulations we run is simple (in principle), but does\n",
            "nothing about using a ˆP which is not the same as the true P. We can’t do much about\n",
            "the systematic error issue either, unless we ﬁnd better models. But there are tricks\n",
            "for addressing the third issue — of trying to bring ˆP closer to P at a given amount of\n",
            "data. One of the most important of these is called pivoting, and it involves changing,\n",
            "slightly, what we look at in the simulations.\n",
            "We have a certain estimate of our quantity of interest from the data, ˆθ; this is\n",
            "a random variable with a distribution governed by P. When we run the bootstrap,\n",
            "we get estimates on the bootstrap data, ˜θ, with a distribution governed by ˆP. So far,\n",
            "we’ve been saying that the distribution of ˜θ should be approximately the same as the\n",
            "distribution of ˆθ. In symbols,\n",
            "�( ˆθ) ≈ �( ˜θ)\n",
            "As the number of actual observations grows, these two distributions should indeed\n",
            "converge. The key observation to pivoting is that, at the same n, the distribution of\n",
            "estimation errors is often closer than the distribution of estimates:\n",
            "�( ˆθ − θ0) ≈ �( ˜θ − ˆθ)\n",
            "In words: the distribution of bootstrap estimates around our real-data estimate is a\n",
            "good approximation to the distribution of estimates around the truth. In fact, it’s\n",
            "usually better than using the distribution of bootstrap values to approximate the dis-\n",
            "tribution of estimates.\n",
            "395\n",
            "\n",
            "\n",
            "Page 396:\n",
            "25.2. IMPROVING THE BOOTSTRAP: STUDENTIZING AND THE\n",
            "DOUBLE BOOTSTRAP\n",
            "396\n",
            "Conﬁdence Intervals\n",
            "This doesn’t change how we get a bootstrap standard error,\n",
            "or a bias. But it does change our bootstrap conﬁdence intervals. Let’s say the quantiles\n",
            "of ˜θ are qα/2 and q1−α/2. In our ﬁrst crude bootstrap1, we’d use the range [qα/2,q1−α/2]\n",
            "as our 1 − α conﬁdence interval for θ. Here’s how we come up with a better interval\n",
            "using pivoting:\n",
            "1 − α\n",
            "=\n",
            "ˆP(qα/2 ≤ ˜θ ≤ q1−α/2)\n",
            "(25.1)\n",
            "=\n",
            "P(qα/2 − ˆθ ≤ ˜θ − ˆθ ≤ q1−α/2 − ˆθ)\n",
            "(25.2)\n",
            "≈\n",
            "P(qα/2 − ˆθ ≤ ˆθ − θ0 ≤ q1−α/2 − ˆθ)\n",
            "(25.3)\n",
            "=\n",
            "P(qα/2 − 2 ˆθ ≤ −θ0 ≤ q1−α/2 − 2 ˆθ)\n",
            "(25.4)\n",
            "=\n",
            "P(2 ˆθ − q1−α/2 ≤ θ0 ≤ 2 ˆθ − qα/2)\n",
            "(25.5)\n",
            "=\n",
            "P( ˆθ + ( ˆθ) − q1−α/2) ≤ θ0 ≤ ˆθ + ( ˆθ − qα/2))\n",
            "(25.6)\n",
            "The third line is where we use the bootstrap principle, and pivoting: the distribution\n",
            "of ˜θ around ˆθ should be close to the distribution of ˆθ around θ0. The rest is just book-\n",
            "keeping, where in the last line I’ve re-written it so it looks more like the conﬁdence\n",
            "intervals we’re used to seeing for means or for regression coefﬁcients.\n",
            "25.2\n",
            "Improving the Bootstrap: Studentizing and the\n",
            "Double Bootstrap\n",
            "25.3\n",
            "A Hint of Theory\n",
            "25.3.1\n",
            "Why the Bootstrap Works\n",
            "25.3.2\n",
            "When the Bootstrap Fails\n",
            "1Often called the “quantile” or “percentile” bootstrap.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 397:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Appendix A\n",
            "Problem Sets\n",
            "Weekly problem sets were an essential part of the class. Add them!\n",
            "397\n",
            "\n",
            "\n",
            "Page 398:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Appendix B\n",
            "TODO\n",
            "• Fix URLs for data sets to something not so class-speciﬁc\n",
            "• Improve cross-chapter cross-references (to individual sections or even pages, not\n",
            "just the chapter as a whole)\n",
            "• Add lasso material to chapter on variable selection\n",
            "• Number all equations\n",
            "• Fix collisions between captions and footers\n",
            "B.1\n",
            "Outline to Be Revised To\n",
            "This is just an aid to my own mem-\n",
            "ory — CRS\n",
            "1. Linear Least Squares with One Predictor\n",
            "(a) Optimal Linear Prediction\n",
            "(b) Estimation by Least Squares\n",
            "(c) Diagnostics\n",
            "(d) Outliers and Inﬂuential Points\n",
            "(e) Parametric Inference: Bootstrap and Asymptotics\n",
            "(f) Predictive Inference\n",
            "(g) Model Selection\n",
            "(h) Smoothing Splines\n",
            "(i) Gaussian Noise Theory\n",
            "2. Linear Least Squares with More Than One Predictor\n",
            "(a) Linear Models in Matrix Form\n",
            "(b) Estimation by Least Squares\n",
            "(c) Polynomial Terms, Categorical Predictors, Interactions\n",
            "398\n",
            "\n",
            "\n",
            "Page 399:\n",
            "399\n",
            "B.1. OUTLINE TO BE REVISED TO\n",
            "(d) Diagnostics\n",
            "(e) Collinearity\n",
            "(f) Inﬂuential Points and Outliers\n",
            "(g) Parametric Inference: Bootstrap and Asymptotics\n",
            "(h) Predictive Inference\n",
            "(i) Variable Selection\n",
            "(j) Linear Prediction and Estimation for Spatio-Temporal Data (Forecasting,\n",
            "Kriging, Filtering)\n",
            "(k) Additive Models\n",
            "(l) Gaussian Noise Theory\n",
            "3. Beyond Ordinary Least Squares\n",
            "(a) Weighted Least Squares and Heteroskedasticity\n",
            "(b) Generalized Least Squares and Correlated Noise\n",
            "(c) Ridge Regression\n",
            "(d) The Lasso\n",
            "(e) Generalized Linear Models\n",
            "(f) Regression Trees\n",
            "4. Let the Dead Bury the Dead\n",
            "(a) What F and Wald Tests Actually Test\n",
            "(b) R2: Distraction or Nuisance?\n",
            "(c) Transforming for Gaussianity\n",
            "(d) ANOVA Tables\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 400:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Acknowledgments\n",
            "Thanks go ﬁrst of all to my students in 36-401, who put up with learning from ﬁrst\n",
            "drafts posted twice a week. I am also grateful to my colleague Prof. Rebecca Nugent,\n",
            "who taught 401 for many years before me, for access to her teaching materials, and\n",
            "permission to borrow some of them for use in my class (none of her materials appear\n",
            "in this text, however), as well as for her sense of humor about the students’ occasional\n",
            "“shenanigans”, and her forbearance towards my own foibles. More broadly, I am in-\n",
            "debted to my colleagues in the Statistics Department at CMU, for creating a program\n",
            "where a class like this could be taught to undergraduates.\n",
            "For speciﬁc corrections, beyond the students in 401, I thank Martin Buttenschön,\n",
            "David Darmon, Dr. Niels Hagenbuch, and Justin Lazarow, and Pedro Enrique Passos.\n",
            "All remaining errors and lapses are, of course, my fault.\n",
            "During the period when I wrote these notes, I was partially supported by grants\n",
            "from the Institute for New Economic Thinking (INO1400020) and from the National\n",
            "Science Foundation (DMS1418124). Those were intended to support my research,\n",
            "but money is fungible, so they have my thanks — but of course neither they, nor my\n",
            "regular employer, is in any way responsible for my opinions here.\n",
            "400\n",
            "\n",
            "\n",
            "Page 401:\n",
            "16:31 Saturday 2nd November, 2019\n",
            "Copyright ©Cosma Rohilla Shalizi; do not distribute without permission\n",
            "updates at http://www.stat.cmu.edu/~cshalizi/TALR/\n",
            "Bibliography\n",
            "Adler, Daniel, Duncan Murdoch and others (2014). rgl: 3D visualization device sys-\n",
            "tem (OpenGL). URL http://CRAN.R-project.org/package=rgl. R package\n",
            "version 0.95.1201.\n",
            "Akaike, Hirotugu (1973). “Information Theory and an Extension of the Maximum\n",
            "Likelihood Principle.” In Proceedings of the Scond International Symposium on Infor-\n",
            "mation Theory (B. N. Petrov and F. Caski, eds.), pp. 267–281. Budapest: Akademiai\n",
            "Kiado. Reprinted in (Akaike, 1998, pp. 199–213).\n",
            "— (1998). Selected Papers of Hirotugu Akaike. Berlin: Springer-Verlag. Edited by\n",
            "Emanuel Parzen, Kunio Tanabe and Genshiro Kitagawa.\n",
            "Alford, J. R., C. L. Funk and J. R. Hibbibng (2005). “Are Political Orientations\n",
            "Genetically Transmitted?” American Political Science Review, 99: 153–167.\n",
            "Anderson, Chris (June 2008). “The End of Theory: The Data Deluge Makes the Sci-\n",
            "entiﬁc Method Obsolete.” Wired, 16(17). URL http://www.wired.com/2008/\n",
            "06/pb-theory/.\n",
            "Anderson, Norman H. and James Shanteau (1977). “Weak inference with linear mod-\n",
            "els.” Psychological Bulletin, 84: 1155–1170. doi:10.1037/0033-2909.84.6.1155.\n",
            "Ashby, F. Gregory (2011).\n",
            "Statistical Analysis of fMRI Data.\n",
            "Cambridge, Mas-\n",
            "sachusetts: MIT Press.\n",
            "Axler, Sheldon (1996). Linear Algebra Done Right. Undergraduate Texts in Mathe-\n",
            "matics. Berlin: Springer-Verlag.\n",
            "Badii, Remo and Antonio Politi (1997). Complexity: Hierarchical Structures and Scal-\n",
            "ing in Physics. Cambridge, England: Cambridge University Press.\n",
            "Benjamini, Yoav and Yosef Hochberg (1995). “Controlling the False Discovery Rate:\n",
            "A Practical and Powerful Approach to Multiple Testing.”\n",
            "Journal of the Royal\n",
            "Statistical Society B, 57: 289–300. URL http://www.math.tau.ac.il/~ybenja/\n",
            "MyPapers/benjamini_hochberg1995.pdf.\n",
            "Bennett, Craig M., Abigail A. Baird, Michael B. Miller and George L. Wolford (2010).\n",
            "“Neural correlates of interspecies perspective taking in the post-mortem Atlantic\n",
            "401\n",
            "\n",
            "\n",
            "Page 402:\n",
            "BIBLIOGRAPHY\n",
            "402\n",
            "Salmon: An argument for multiple comparisons correction.” Journal of Serendip-\n",
            "itous and Unexpected Results, 1: 1–5.\n",
            "URL http://prefrontal.org/files/\n",
            "posters/Bennett-Salmon-2009.pdf.\n",
            "Berk, Richard A. (2004). Regression Analysis: A Constructive Critique. Thousand\n",
            "Oaks, California: Sage.\n",
            "Bettencourt, Luís M. A., José Lobo, Dirk Helbing, Christian Künhert and Ge-\n",
            "offrey B. West (2007).\n",
            "“Growth, Innovation, Scaling, and the Pace of Life in\n",
            "Cities.” Proceedings of the National Academy of Sciences (USA), 104: 7301–7306.\n",
            "doi:10.1073/pnas.0610172104.\n",
            "Birnbaum, Michael H. (1973). “The Devil Rides Again: Correlation as an Index of\n",
            "Fit.” Psychological Bulletin, 79: 239–242. doi:10.1037/h0033853.\n",
            "Bühlmann, Peter (2014). “High-Dimensional Statistics with a View Toward Appli-\n",
            "cations in Biology.” Annual Review of Statistics and Its Applications, 1: 255–278.\n",
            "doi:10.1146/annurev-statistics-022513-115545.\n",
            "Bühlmann, Peter and Sara van de Geer (2011). Statistics for High-Dimensional Data:\n",
            "Methods, Theory and Applications. Berlin: Springer-Verlag.\n",
            "Buja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin,\n",
            "Mikhail Traskin, Linda Zhao and Kai Zhang (2014). “Models as Approximations:\n",
            "A Conspiracy of Random Regressors and Model Deviations Against Classical Infer-\n",
            "ence in Regression.” arxiv:1404.1578. URL http://arxiv.org/abs/1404.1578.\n",
            "Casella, George and R. L. Berger (2002). Statistical Inference. Belmont, California:\n",
            "Duxbury Press, 2nd edn.\n",
            "Cramér, Harald (1945). Mathematical Methods of Statistics. Uppsala: Almqvist and\n",
            "Wiksells.\n",
            "Cule, Erika (2014). ridge: Ridge Regression with automatic selection of the penalty\n",
            "parameter. URL http://CRAN.R-project.org/package=ridge. R package ver-\n",
            "sion 2.1-3.\n",
            "Davison, A. C. and D. V. Hinkley (1997). Bootstrap Methods and their Applications.\n",
            "Cambridge, England: Cambridge University Press.\n",
            "Dhillon, Paramveer S., Dean P. Foster, Sham M. Kakade and Lyle H. Ungar (2013).\n",
            "“A Risk Comparison of Ordinary Least Squares vs Ridge Regression.” Journal of\n",
            "Machine Lerning Research, 14: 1505–1511. URL http://jmlr.org/papers/v14/\n",
            "dhillon13a.html.\n",
            "DuMouchel, William H. and Greg J. Duncan (1983). “Using Sample Survey Weights\n",
            "in Multiple Regression Analyses of Stratiﬁed Samples.”\n",
            "Journal of the Ameri-\n",
            "can Statistical Association, 78: 535–543. URL http://www.jstor.org/stable/\n",
            "2288115.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 403:\n",
            "403\n",
            "BIBLIOGRAPHY\n",
            "Efron, Bradley (1979).\n",
            "“Bootstrap Methods: Another Look at the Jackknife.”\n",
            "Annals of Statistics, 7: 1–26. URL http://projecteuclid.org/euclid.aos/\n",
            "1176344552.\n",
            "Feller, William (1957). An Introduction to Probability Theory and Its Applications,\n",
            "vol. I. New York: Wiley, 2nd edn.\n",
            "Feynman, Richard P. (1985). QED: The Strange Theory of Light and Matter. Princeton,\n",
            "New Jersey: Princeton University Press.\n",
            "Freedman, David A. (1983). “A Note on Screening Regression Equations.” The Amer-\n",
            "ican Statistician, 37: 152–155. doi:10.1080/00031305.1983.10482792.\n",
            "Galambos, Janos and Italo Simonelli (1996). Bonferroni-type Inequalities with Applica-\n",
            "tions. Berlin: Springer-Verlag.\n",
            "Gelman, Andrew (2005). “Analysis of Variance — Why It Is More Important than\n",
            "Ever.” Annals of Statistics, 33: 1–53. URL http://projecteuclid.org/euclid.\n",
            "aos/1112967698. doi:10.1214/009053604000001048.\n",
            "Genovese,\n",
            "Christopher and Larry Wasserman (2004).\n",
            "“A Stochastic Pro-\n",
            "cess Approach to False Discovery Control.”\n",
            "Annals of Statistics,\n",
            "32:\n",
            "1035–1061.\n",
            "URL http://projecteuclid.org/euclid.aos/1085408494.\n",
            "doi:10.1214/009053604000000283.\n",
            "Gouriéroux, Christian and Alain Monfort (1989/1995).\n",
            "Statistics and Economet-\n",
            "ric Models.\n",
            "Themes in Modern Econometrics. Cambridge, England:\n",
            "Cam-\n",
            "bridge University Press. Translated by Quang Vuong from Statistique et modèles\n",
            "économétriques, Paris: Économica.\n",
            "Hamilton, Richard F. (1996). The Social Misconstruction of Reality: Validity and Veri-\n",
            "ﬁcation in the Scholarly Community. New Haven: Yale University Press.\n",
            "Hoerl, Arthur E. and Robert W. Kennard (1970). “Ridge Regression: Biased Estima-\n",
            "tion for Nonorthogonal Problems.” Technometrics, 12. URL http://www.jstor.\n",
            "org/pss/1267351.\n",
            "Hoover, Kevin D. and Mark V. Siegler (2008). “Sound and Fury: McCloskey and\n",
            "Signiﬁcance Testing in Economics.” Journal of Economic Methodology, 15: 1–37.\n",
            "URL http://hdl.handle.net/10161/2045. doi:10.1080/13501780801913298.\n",
            "Klein,\n",
            "Dan (2001).\n",
            "“Lagrange Multipliers without Permanent Scarring.”\n",
            "Online\n",
            "tutorial.\n",
            "URL\n",
            "http://dbpubs.stanford.edu:8091/~klein/\n",
            "lagrange-multipliers.pdf.\n",
            "Li, Ming and Paul M. B. Vitányi (1997). An Introduction to Kolmogorov Complexity\n",
            "and Its Applications. New York: Springer-Verlag, 2nd edn.\n",
            "Li, Qi and Jeffrey Scott Racine (2007). Nonparametric Econometrics: Theory and Prac-\n",
            "tice. Princeton, New Jersey: Princeton University Press.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 404:\n",
            "BIBLIOGRAPHY\n",
            "404\n",
            "Low-Décarie, Etienne, Corey Chivers and Monica Granados (2014). “Rising com-\n",
            "plexity and falling explanatory power in ecology.” Frontiers in Ecology and the\n",
            "Environment, 12: 412–418. doi:10.1890/130230.\n",
            "Mayo, Deborah G. (1996). Error and the Growth of Experimental Knowledge. Chicago:\n",
            "University of Chicago Press.\n",
            "Mayo, Deborah G. and D. R. Cox (2006). “Frequentist Statistics as a Theory of In-\n",
            "ductive Inference.” In Optimality: The Second Erich L. Lehmann Symposium (Javier\n",
            "Rojo, ed.), pp. 77–97. Bethesda, Maryland: Institute of Mathematical Statistics.\n",
            "URL http://arxiv.org/abs/math.ST/0610846.\n",
            "Mayo, Deborah G. and Aris Spanos (2006). “Severe Testing as a Basic Concept in a\n",
            "Neyman-Pearson Philosophy of Induction.” The British Journal for the Philosophy\n",
            "of Science, 57: 323–357. doi:10.1093/bjps/axl003.\n",
            "McCloskey, D. N. (2002). The Secret Sins of Economics. Chicago: Prickly Paradigm\n",
            "Press. URL www.prickly-paradigm.com/paradigm4.pdf.\n",
            "McCloskey, D. N. and S. T. Ziliak (1996). “The Standard Error of Regressions.” Jour-\n",
            "nal of Economic Literature, 34: 97–114.\n",
            "Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller and E. Teller\n",
            "(1953). “Equations of State Calculations by Fast Computing Machines.” Journal of\n",
            "Chemical Physics, 21: 1087–1092. doi:10.1063/1.1699114.\n",
            "Murdoch, Duncan and E. D. Chow (2013). ellipse: Functions for drawing ellipses\n",
            "and ellipse-like conﬁdence regions. URL http://CRAN.R-project.org/package=\n",
            "ellipse. R package version 0.3-8.\n",
            "Neyman, Jerzy (1937). “Outline of a Theory of Statistical Estimation Based on the\n",
            "Classical Theory of Probability.” Philosophical Transactions of the Royal Society of\n",
            "London A, 236: 333–380. doi:10.1098/rsta.1937.0005.\n",
            "Petersen, Kaare Brandt and Michael Syskind Pedersen (2012).\n",
            "The Matrix Cook-\n",
            "book.\n",
            "Tech. rep., Technical University of Denmark, Intelligent Signal Pro-\n",
            "cessing Group. URL http://www2.imm.dtu.dk/pubdb/views/publication_\n",
            "details.php?id=3274.\n",
            "Pitman, E. J. G. (1979). Some Basic Theory for Statistical Inference. London: Chapman\n",
            "and Hall.\n",
            "Quiñonero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer and Neil D.\n",
            "Lawrence (eds.) (2009).\n",
            "Dataset Shift in Machine Learning.\n",
            "Cambridge, Mas-\n",
            "sachusetts: MIT Press.\n",
            "Ruelle, David (1991). Chance and Chaos. Princeton, New Jersey: Princeton Univer-\n",
            "sity Press.\n",
            "Salmon, Wesley C. (1984). Scientiﬁc Explanation and the Causal Structure of the World.\n",
            "Princeton: Princeton University Press.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 405:\n",
            "405\n",
            "BIBLIOGRAPHY\n",
            "Schervish, Mark J. (1995). Theory of Statistics. Berlin: Springer-Verlag.\n",
            "Schwarz, Gideon (1978). “Estimating the Dimension of a Model.” Annals of Statistics,\n",
            "6: 461–464. URL http://projecteuclid.org/euclid.aos/1176344136.\n",
            "Seber, George A. F. and Alan J. Lee (2003). Linear Regression Analysis. New York:\n",
            "Wiley, 2nd edn.\n",
            "Serber, Robert (1992). The Los Alamos Primer: The First Lectures on How to Build\n",
            "the Atomic Bomb. Berkeley: University of California Press. Annotated by Robert\n",
            "Serber; edited and with an introduction by Richard Rhodes.\n",
            "Shalizi, Cosma Rohilla (forthcoming). Advanced Data Analysis from an Elementary\n",
            "Point of View. Cambridge, England: Cambridge University Press. URL http:\n",
            "//www.stat.cmu.edu/~cshalizi/ADAfaEPoV.\n",
            "Shmueli, Galit (2010). “To Explain or to Predict?” Statistical Science, 25: 289–310.\n",
            "doi:10.1214/10-STS330.\n",
            "Simonoff, Jeffrey S. (1996). Smoothing Methods in Statistics. Berlin: Springer-Verlag.\n",
            "Smith, Leonard (2007). Chaos: A Very Short Introduction. Oxford: Oxford University\n",
            "Press.\n",
            "Swift, Jonathan (1726). Gulliver’s Travels. London: Benjamin Motte. URL http:\n",
            "//www.gutenberg.org/ebooks/829. Originally published as Travels into Several\n",
            "Remote Nations of the World. In Four Parts. By Lemuel Gulliver, First a Surgeon, and\n",
            "then a Captain of several Ships.\n",
            "Thompson, Wilbur R. (1968). Preface to Urban Economics. Baltimore: Johns Hopkins\n",
            "University Press.\n",
            "Tukey, John W. (1954). “Unsolved Problems of Experimental Statistics.” Journal of\n",
            "the American Statistical Association, 49: 706–731. URL http://www.jstor.org/\n",
            "pss/2281535.\n",
            "Tutz, Gerhard (2012). Regression for Categorical Data. Cambridge, England: Cam-\n",
            "bridge University Press.\n",
            "van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge, England: Cambridge\n",
            "University Press.\n",
            "Vuong, Quang H. (1989). “Likelihood Ratio Tests for Model Selection and Non-\n",
            "Nested Hypotheses.” Econometrica, 57: 307–333. URL http://www.jstor.org/\n",
            "pss/1912557.\n",
            "Wainwright, Martin J. (2014). “Structured Regularizers for High-Dimensional Prob-\n",
            "lems: Statistical and Computational Issues.” Annal Review of Statistics and Its Ap-\n",
            "plications, 1: 233–253. doi:10.1146/annurev-statistics-022513-115643.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n",
            "Page 406:\n",
            "BIBLIOGRAPHY\n",
            "406\n",
            "Ward, Michael D., Brian D. Greenhill and Kristin M. Bakke (2010). “The Perils of\n",
            "Policy by p-value: Predicting Civil Conﬁcts.” Journal of Peace Research, 47: 363–\n",
            "375. doi:10.1177/0022343309356491.\n",
            "Wasserman, Larry (2006). All of Nonparametric Statistics. Berlin: Springer-Verlag.\n",
            "Weisburd, David and Alex R. Piquero (2008). “How Well Do Criminologists Explain\n",
            "Crime? Statistical Modeling in Published Studies.” Crime and Justice, 37: 453–502.\n",
            "doi:10.1086/524284.\n",
            "Wilks, S. S. (1938).\n",
            "“The Large Sample Distribution of the Likelihood Ra-\n",
            "tio for Testing Composite Hypotheses.”\n",
            "Annals of Mathematical Statistics,\n",
            "9:\n",
            "60–62.\n",
            "URL http://projecteuclid.org/euclid.aoms/1177732360.\n",
            "doi:10.1214/aoms/1177732360.\n",
            "Winship, Christopher and Robert D. Mare (1984). “Regression Models with Ordinal\n",
            "Variables.” American Sociological Review, 49: 512–525. URL http://scholar.\n",
            "harvard.edu/files/cwinship/files/asr_1984.pdf.\n",
            "16:31 Saturday 2nd November, 2019\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}